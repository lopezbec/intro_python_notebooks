{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><b>©Content is made available under the CC-BY-NC-ND 4.0 license. Christian Lopez, lopezbec@lafayette.edu<center>"
      ],
      "metadata": {
        "id": "KNHJp8fYKRex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Intro to Machine Learning Capstone Deliverable**\n",
        "\n",
        "For this deliverable, you will work on two parts, which contain the following sections:\n",
        "\n",
        "**Part 1: Regression**\n",
        "- 1.1 Generate a synthetic dataset.\n",
        "- 1.2 Split and scale the data.\n",
        "- 1.3 Build linear regression models using polynomial features of increasing degree to observe underfitting versus overfitting.\n",
        "- 1.4 Apply Ridge regularization to see its effect on an overfitting model.\n",
        "\n",
        "**Part 2: Classification**\n",
        "- 2.1 Load an image dataset for classification.\n",
        "- 2.2 Split and scale the data.\n",
        "- 2.3 Train a \"*simple*\" feedforward Neural Network.\n",
        "- 2.4 Train a \"*deeper*\" feedforward Neural Network.\n",
        "- 2.5 Show the performance of our models on two different datasets.\n"
      ],
      "metadata": {
        "id": "MsirdR0gNW91"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-assignment"
      },
      "source": [
        "## **Video Instructions:**\n",
        "\n",
        "For your video, imagine you are creating a tutorial to explain the basics of Machine Learning to others. Your video will show you going over your completed notebook and explaining all of its parts (no need to explain the code, just what is being done). You are encouraged to use GenAI to help you better understand the concepts. However, avoid using GenAI to create a script and simply read from it; try to learn the concepts and use your own words! Record a video explaining:\n",
        "\n",
        "1. Why data splitting and scaling are necessary.\n",
        "2. The difference between underfitting and overfitting, and how you can detect these issues.\n",
        "3. How increasing the polynomial degree (i.e., model complexity) impacts model performance (underfitting vs. overfitting).\n",
        "4. How Ridge regularization helps reduce overfitting.\n",
        "5. Why there is a performance difference between a \"simple\" neural network and a \"deeper\" neural network.\n",
        "6. Why there is a performance difference when using the same models on a more complex image dataset.\n",
        "\n",
        "You do not need to type your answers in the notebook—explain these concepts in your video using the plots, performance metrics, and any outputs from the code chunks in your notebook to support your explanations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video-instructions"
      },
      "source": [
        "## **Coding Instructions:**\n",
        "\n",
        "You are allowed to use GenAI to complete the code chunks (Google Colab has Copilot integrated). There will be some code chunks that need to be completed, so make sure to read the instructions carefully since your code needs to follow them (e.g., how to name things, etc.). Only modify/type your code within the comments:\n",
        "\n",
        "`### START CODE HERE ###`\n",
        "\n",
        "`### END CODE HERE ###`\n",
        "\n",
        "Modifying any other code or not following the instructions/comments may potentially create errors in other areas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "\n",
        "Before starting anything, we need to make sure to import all the necessary libraries we will use in this notebook\n"
      ],
      "metadata": {
        "id": "OXLGlQpsO3VP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "provided-imports"
      },
      "source": [
        "# Provided code: Import necessary libraries for regression and classification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# For classification using Keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "partA-intro"
      },
      "source": [
        "## Part 1: Regression\n",
        "\n",
        "In this section, we will:\n",
        "- 1.1 Generate a synthetic dataset.\n",
        "- 1.2 Split and scale the data.\n",
        "- 1.3 Train polynomial regression models with degrees 1, 5, and 20.\n",
        "- 1.4 Apply Ridge regularization to see its effect on an overfitting model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1 Generate a synthetic dataset."
      ],
      "metadata": {
        "id": "gXxByL34PpfQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "regression-generate-data"
      },
      "source": [
        "#Generate synthetic regression data\n",
        "np.random.seed(42)  # for reproducibility\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = 0.5 * X**3 - X**2 + np.random.randn(100, 1) * 3\n",
        "\n",
        "print('Synthetic regression data generated.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have generated our synthetic data, let's look at it and see some summary statistics.\n"
      ],
      "metadata": {
        "id": "5eVh0eWtQKbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the synthetic regression data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X, y, color='blue', label='Data points')\n",
        "plt.title('Synthetic Regression Data')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9vAiunOBPxdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine X and y into a single DataFrame\n",
        "df = pd.DataFrame(np.hstack((X, y)), columns=['X', 'y'])\n",
        "# Display summary statistics using the describe() method\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "uPloRc27Qefi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Split and scale the data."
      ],
      "metadata": {
        "id": "nIj00QrxQThL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1 Split data\n",
        "\n",
        "Here we will just split the data between a training set and a test set. No validation set is needed since no hyperparameter tuning will be performed. We will use an 80/20 partition.\n"
      ],
      "metadata": {
        "id": "ke4jZTcjaQTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split and scale the regression data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "JEVf0YExagnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remember to explain why do we need to split out data**\n",
        "\n",
        "\n",
        "You could also talk about other ways to splitting the data, and/or why 80/20, etc\n"
      ],
      "metadata": {
        "id": "1XghNmU1ah1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1 Scale data"
      ],
      "metadata": {
        "id": "DQTh3H_iaVfm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "regression-split-scale-data"
      },
      "source": [
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "\n",
        "### START CODE HERE ### (≈ 4 lines of code)\n",
        "\n",
        "X_train_scaled =\n",
        "X_test_scaled =\n",
        "\n",
        "y_train_scaled =\n",
        "y_test_scaled =\n",
        "\n",
        "### END CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is just to see how your data looks"
      ],
      "metadata": {
        "id": "kZ8isaexEIDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_or = pd.DataFrame(X_train, columns=['X_train'])\n",
        "df_or['y_train'] = y_train\n",
        "print(df_or.describe())\n",
        "\n",
        "df_sc = pd.DataFrame(X_train_scaled, columns=['X_train_scaled'])\n",
        "df_sc['y_train_scaled'] = y_train_scaled\n",
        "print(df_sc.describe())\n",
        "\n"
      ],
      "metadata": {
        "id": "tYwZ_h1rDrI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "\n",
        "The numbers above should look something like:\n",
        "```\n",
        " X_train    y_train\n",
        "count  80.000000  80.000000\n",
        "mean    0.075758  -3.132195\n",
        "std     1.760668   6.215460\n",
        "min    -2.939394 -21.753065\n",
        "25%    -1.439394  -6.253508\n",
        "50%     0.121212  -1.297410\n",
        "75%     1.500000   1.260989\n",
        "max     3.000000   4.535356\n",
        "       X_train_scaled  y_train_scaled\n",
        "count    8.000000e+01    8.000000e+01\n",
        "mean     2.775558e-17   -1.387779e-17\n",
        "std      1.006309e+00    1.006309e+00\n",
        "min     -1.723309e+00   -3.014798e+00\n",
        "25%     -8.659843e-01   -5.053538e-01\n",
        "50%      2.597953e-02    2.970594e-01\n",
        "75%      8.140252e-01    7.112751e-01\n",
        "max      1.671350e+00    1.241409e+00\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "y1WeaEM3ELzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remember to explain why do we need to scale our data?**\n",
        "\n",
        "\n",
        " When we have multiple features, scaling becomes potentially more important. Scaling you target varialbe is not as critical, but a good practice. Also, try to explain why type of scaling was performed (i.e., look at the `std` of the `X_train_scaled` vs `X_train`.  Also, why do we use  `scaler_X.fit_transform(....)` for the `X_train_scaled`, but for the `X_test_scaled` we used `scaler_X.transform(...)`\n"
      ],
      "metadata": {
        "id": "ZwVB_4DLEvtR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poly-regression-explanation"
      },
      "source": [
        "### 1.3 Train polynomial regression models with degrees 1, 5, and 20.\n",
        "\n",
        "After training each model, the performance (R² score) is computed on both the training and test sets to observe the effect of model complexity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Polynomial regression models with degrees=1\n",
        "\n",
        "Your task is to build a linear model using degree 1 polynomial features (which is just your original data with an intercept). Follow these steps:\n",
        "\n",
        "1. Create a `PolynomialFeatures` object with `degree=1`.\n",
        "2. Use `fit_transform()` on your scaled training data.\n",
        "3. Initialize and fit a `LinearRegression` model using the transformed features and `y_train_scaled`.\n"
      ],
      "metadata": {
        "id": "s5wBSIIIKDLG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poly-degree-1-training"
      },
      "source": [
        "### START CODE HERE ### (≈ 4 lines of code)\n",
        "poly1 =\n",
        "X_train_poly1 =\n",
        "\n",
        "model1 =\n",
        "model1.fit(X_train_poly1, y_train_scaled)\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poly-degree-1-plot"
      },
      "source": [
        "# Provided code: Plot predictions for polynomial degree 1\n",
        "X_plot = np.linspace(X_train_scaled.min(), X_train_scaled.max(), 100).reshape(-1, 1)\n",
        "X_plot_poly1 = poly1.transform(X_plot)\n",
        "y_plot1 = model1.predict(X_plot_poly1)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_train_scaled, y_train_scaled, color='blue', label='Training Data')\n",
        "plt.plot(X_plot, y_plot1, color='red', label='Degree 1 Prediction')\n",
        "plt.title('Polynomial Degree 1')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poly-degree-1-performance"
      },
      "source": [
        "# Provided performance evaluation for polynomial degree 1\n",
        "train_score_deg1 = model1.score(X_train_poly1, y_train_scaled)\n",
        "test_score_deg1 = model1.score(poly1.transform(X_test_scaled), y_test_scaled)\n",
        "print('Degree 1 - Training R²:', train_score_deg1, ' | Test R²:', test_score_deg1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Degree 1 - Training R²: 0.6065406392361405  | Test R²: 0.595089539490184\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "zs0GHsstAR_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Polynomial regression models with degrees=5\n",
        "\n",
        "Your task is to build a linear model using degree 5 polynomial features. Follow these steps:\n",
        "\n",
        "1. Create a `PolynomialFeatures` object with `degree=5` and use `fit_transform()` on your already scaled training data.\n",
        "\n",
        "2. Create a new `StandardScaler` and use it to scale the polynomial features (even though we are using the already scaled data, when increasing the degree of the polynomial, the range of the new features might be very different, so we scale them again).\n",
        "\n",
        "3. Initialize a `LinearRegression` model and fit it on the scaled polynomial features and the scaled target.\n"
      ],
      "metadata": {
        "id": "9Jpnway3LHvW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poly-degree-3-training"
      },
      "source": [
        "### START CODE HERE ### (≈ 6 lines of code)\n",
        "# Train model with polynomial degree 5\n",
        "poly5 =\n",
        "X_train_poly5 =\n",
        "\n",
        "# Scale the polynomial features\n",
        "scaler_poly5 =\n",
        "X_train_poly5_scaled =\n",
        "\n",
        "model5 =\n",
        "model5.fit(X_train_poly5_scaled, y_train_scaled)\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poly-degree-3-plot"
      },
      "source": [
        "# Provided code: Plot predictions for polynomial degree 5\n",
        "X_plot_poly5 = poly5.transform(X_plot)\n",
        "X_plot_poly5_scaled = scaler_poly5.transform(X_plot_poly5)  # Scale the polynomial features for the plot\n",
        "y_plot5 = model5.predict(X_plot_poly5_scaled)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_train_scaled, y_train_scaled, color='blue', label='Training Data')\n",
        "plt.plot(X_plot, y_plot5, color='red', label='Degree 5 Prediction')\n",
        "plt.title('Polynomial Degree 5')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poly-degree-3-performance"
      },
      "source": [
        "# Provided performance evaluation for polynomial degree 5\n",
        "# Use the already scaled polynomial features for the training set\n",
        "train_score_deg5 = model5.score(X_train_poly5_scaled, y_train_scaled)\n",
        "# Transform and then scale the test data using the same poly5 and scaler_poly5\n",
        "X_test_poly5 = poly5.transform(X_test_scaled)\n",
        "X_test_poly5_scaled = scaler_poly5.transform(X_test_poly5)\n",
        "test_score_deg5 = model5.score(X_test_poly5_scaled, y_test_scaled)\n",
        "print('Degree 5 - Training R²:', train_score_deg5, ' | Test R²:', test_score_deg5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Degree 5 - Training R²: 0.8119308456491077  | Test R²: 0.8765925052424351\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZhQK6ccEAYPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Polynomial regression models with degrees=20\n",
        "\n",
        "Your task is to build a linear model using degree 20 polynomial features; however, now we will use a `Pipeline` object to make things a bit easier. Follow these steps:\n",
        "\n",
        "1. Create a `Pipeline` from scikit-learn.\n",
        "2. Add the `PolynomialFeatures(degree=20)` to the Pipeline (name this step `'poly'`).\n",
        "3. Add `StandardScaler()` to the Pipeline (name this step `'scaler'`).\n",
        "4. Add `LinearRegression()` to the Pipeline (name this step `'lin_reg'`).\n",
        "5. Use the already scaled training data to fit the entire pipeline.\n"
      ],
      "metadata": {
        "id": "xSugfiZr9EBB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poly-degree-10-training"
      },
      "source": [
        "### START CODE HERE ###\n",
        "\n",
        "# Create a pipeline for a degree 20 polynomial regression model\n",
        "pipeline_20 = Pipeline([\n",
        "    ('poly',                 ),\n",
        "    ('scaler',               ),\n",
        "    ('lin_reg',              )\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the already scaled training data and the scaled target\n",
        "pipeline_20.fit(X_train_scaled, y_train_scaled)\n",
        "### END CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poly-degree-10-plot"
      },
      "source": [
        "# Provided code: Plot predictions for polynomial degree 20 using the pipeline\n",
        "# Transform X_plot using the 'poly' and 'scaler' steps from the pipeline\n",
        "X_plot_poly20 = pipeline_20.named_steps['poly'].transform(X_plot)\n",
        "X_plot_poly20_scaled = pipeline_20.named_steps['scaler'].transform(X_plot_poly20)\n",
        "y_plot20 = pipeline_20.named_steps['lin_reg'].predict(X_plot_poly20_scaled)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_train_scaled, y_train_scaled, color='blue', label='Training Data')\n",
        "plt.plot(X_plot, y_plot20, color='red', label='Degree 20 Prediction')\n",
        "plt.title('Polynomial Degree 20')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poly-degree-10-performance"
      },
      "source": [
        "# Provided performance evaluation for polynomial degree 20 using the pipeline\n",
        "train_score_deg20 = pipeline_20.score(X_train_scaled, y_train_scaled)\n",
        "test_score_deg20 = pipeline_20.score(X_test_scaled, y_test_scaled)\n",
        "print('Degree 20 - Training R²:', train_score_deg20, '| Test R²:', test_score_deg20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "```\n",
        "Degree 20 - Training R²: 0.836413730721834 | Test R²: -0.14913407267650958\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "MNlBJxKKAcvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Summary of Models performance\n"
      ],
      "metadata": {
        "id": "VHjC0Yhl-WEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Degree 1 - Training R²:', train_score_deg1, ' | Test R²:', test_score_deg1)\n",
        "print('Degree 5 - Training R²:', train_score_deg5, ' | Test R²:', test_score_deg5)\n",
        "print('Degree 20 - Training R²:', train_score_deg20, '| Test R²:', test_score_deg20)"
      ],
      "metadata": {
        "id": "6-uJ5EHV-WZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remember to explain which model is underfitting and which one is overfitting, as well as how you can tell which one is doing what**\n",
        "\n",
        "\n",
        " Make sure to use the performance numbers and plots to help your explanations.\n"
      ],
      "metadata": {
        "id": "1i7Y8miJAkbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Apply Ridge regularization to see its effect on our degree=20 model.\n",
        "\n",
        "Your task is to build a Ridge regularization linear model using degree 20 polynomial features. We can use a very similar `Pipeline` as the step above:\n",
        "\n",
        "1. I have created a `Pipeline` from scikit-learn.\n",
        "2. I have added the `PolynomialFeatures(degree=20)` to the Pipeline.\n",
        "3. I have added `StandardScaler()` to the Pipeline.\n",
        "4. **Now you replace `LinearRegression()` with `Ridge(alpha=1.0)` to incorporate L2 regularization (name this step `'ridge_reg'`).**\n",
        "5. Use the already scaled training data to fit the entire pipeline.\n"
      ],
      "metadata": {
        "id": "jCCTfbeL-Enf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ridge-regularization"
      },
      "source": [
        "### START CODE HERE ###\n",
        "\n",
        "# Create a pipeline for a degree 20 polynomial regression model with L2 regularization using Ridge regression\n",
        "pipeline_20_reg = Pipeline([\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                            # L2 regularization via Ridge regression (adjust alpha as needed)\n",
        "])\n",
        "\n",
        "pipeline_20_reg.fit(X_train_scaled, y_train_scaled)\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ridge-regularization-plot"
      },
      "source": [
        "# Provided code: Plot Ridge regularized predictions for degree 20 using the pipeline\n",
        "y_plot_ridge = pipeline_20_reg.predict(X_plot)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_train_scaled, y_train_scaled, color='blue', label='Training Data')\n",
        "plt.plot(X_plot, y_plot_ridge, color='green', label='Ridge Regularized Prediction')\n",
        "plt.title('Ridge Regularization on Degree 20 Model')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ridge-regularization-performance"
      },
      "source": [
        "# Provided performance evaluation for the Ridge regularized model using the pipeline\n",
        "train_score_ridge = pipeline_20_reg.score(X_train_scaled, y_train_scaled)\n",
        "test_score_ridge = pipeline_20_reg.score(X_test_scaled, y_test_scaled)\n",
        "print('Ridge Model - Training R²:', train_score_ridge, '| Test R²:', test_score_ridge)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set display option to avoid scientific notation\n",
        "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
        "\n",
        "# Extract feature names from the polynomial transformation in the Ridge pipeline\n",
        "feature_names = pipeline_20_reg.named_steps['poly'].get_feature_names_out()\n",
        "\n",
        "# Extract coefficients from both models\n",
        "coef_ridge = pipeline_20_reg.named_steps['ridge_reg'].coef_.flatten()\n",
        "coef_linear = pipeline_20.named_steps['lin_reg'].coef_.flatten()\n",
        "\n",
        "# Create a DataFrame with coefficients as rows and model names as columns\n",
        "df_coef = pd.DataFrame({\n",
        "    'Ridge': coef_ridge,\n",
        "    'Linear': coef_linear\n",
        "}, index=feature_names)\n",
        "\n",
        "print(\"Theta parameter values (coefficients) for each model:\")\n",
        "print(df_coef)\n",
        "\n"
      ],
      "metadata": {
        "id": "rEz06Vt4ElvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remember to explain what is regularization, and why do we need it**\n",
        "\n",
        " Make sure to use the performance numbers and the coefficients above to help your explanations.\n"
      ],
      "metadata": {
        "id": "lN6NRJkjE9po"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "partB-intro"
      },
      "source": [
        "## Part 2: Classification with Image\n",
        "\n",
        "In this section, you will use the MNIST dataset (handwritten digits) for classification. We will:\n",
        "\n",
        "2.1. Load and split the data.\n",
        "\n",
        "2.2. Scale and flatten the images for Logistic Regression.\n",
        "\n",
        "2.3. Build a \"*simple*\" feedforward neural network using Keras.\n",
        "\n",
        "2.4. Build a \"*deeper*\" feedforward neural network using Keras.\n",
        "\n",
        "2.5. Show the performance of our models on the MNIST fashion dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1 Load and split the data."
      ],
      "metadata": {
        "id": "oTMX4i4MMYqd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnist-load-data"
      },
      "source": [
        "# Provided code: Load the MNIST dataset\n",
        "print('Loading MNIST dataset...')\n",
        "(X_train_img, y_train_img), (X_test_img, y_test_img) = mnist.load_data()\n",
        "\n",
        "# Print shapes to validate\n",
        "print('MNIST training data shape:', X_train_img.shape)\n",
        "print('MNIST test data shape:', X_test_img.shape)\n",
        "plt.figure(figsize=(6,6))\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_train_img[i], cmap='gray')\n",
        "    plt.title(f\"Label: {y_train_img[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.suptitle('Sample MNIST Training Images')\n",
        "plt.show()\n",
        "\n",
        "X_train_img[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains 28 by 28 pixel images of digits. In total, there are 70,000 images of handwritten digits. If you click on the `show data` button above, you will see the raw pixel data. Each digit is represented as a 28 by 28 matrix with grayscale pixel intensity. Notice that the range of pixel values is from 0 to 255.\n"
      ],
      "metadata": {
        "id": "VnPr7uxnMi-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Scale and Flatten the images for Logistic Regression.\n"
      ],
      "metadata": {
        "id": "m48fDXrIMw4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, we will scale our image dataset using min-max normalization, not the z-score normalization provided by the `StandardScaler` method. This is because we already know that the range of our data will always be between 0 and 255, and it also demonstrates a different type of scaling.\n"
      ],
      "metadata": {
        "id": "QHeLRTrfRezT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnist-logistic-prep"
      },
      "source": [
        "\n",
        "X_train_img_scaled = X_train_img/255\n",
        "X_test_img_scaled = X_test_img/255\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnist-keras-explanation"
      },
      "source": [
        "### 2.3. Build a \"*simple*\" feedforward neural network using Keras\n",
        "\n",
        "Now build a simple feedforward neural network using Keras. This network should have:\n",
        "- A Flatten layer to convert the images to vectors.\n",
        "- A Dense hidden layer with 50 neurons and ReLU activation.\n",
        "- A Dense output layer with 10 neurons and softmax activation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnist-keras-training"
      },
      "source": [
        "### START CODE HERE ###\n",
        "# Build a simple feedforward neural network using Keras\n",
        "modelSD= Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Convert images to 1D vector\n",
        "                                  ,   # Hidden layer\n",
        "    Dense(10, activation='softmax')  # Output layer for 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "modelSD.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model (using 10% of the training data as validation set)\n",
        "history = modelSD.fit(X_train_img_scaled, y_train_img, epochs=5, batch_size=32, validation_split=0.1)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print('Keras Neural Network classifier trained on MNIST.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnist-keras-performance"
      },
      "source": [
        "# Provided performance evaluation for Keras Neural Network on MNIST\n",
        "train_loss_nnSD, train_acc_nnSD = modelSD.evaluate(X_train_img_scaled, y_train_img, verbose=0)\n",
        "test_loss_nnSD, test_acc_nnSD = modelSD.evaluate(X_test_img_scaled, y_test_img, verbose=0)\n",
        "print('Simple NN with Digits dataset - Training Accuracy:', train_acc_nnSD, '| Test Accuracy:', test_acc_nnSD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approx. Expected output:**\n",
        "```\n",
        "Simple NN with Digits dataset - Training Accuracy: 0.9776666760444641 | Test Accuracy: 0.9686999917030334\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "V99GQxh4uswk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Build a \"*deeper*\" feedforward neural network using Keras\n",
        "\n",
        "Now build a deeper feedforward neural network using Keras. This network should have:\n",
        "- A Flatten layer to convert the images to vectors.\n",
        "- Three Dense hidden layers with 400, 300, and 200 neurons with ReLu activation, respectively.\n",
        "- A Dense output layer with 10 neurons with softmax activation.\n"
      ],
      "metadata": {
        "id": "t6x9paxRyijI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "# Build a simple feedforward neural network using Keras\n",
        "modelDD = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Convert images to 1D vector\n",
        "                                 ,   # Hidden layer\n",
        "                                 ,   # Hidden layer\n",
        "                                 ,   # Hidden layer\n",
        "    Dense(10, activation='softmax')  # Output layer for 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "modelDD.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model (using 10% of the training data as validation set)\n",
        "history = modelDD.fit(X_train_img_scaled, y_train_img, epochs=5, batch_size=32, validation_split=0.1)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print('Keras Neural Network classifier trained on MNIST.')"
      ],
      "metadata": {
        "id": "YtoN22r0yzON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provided performance evaluation for Keras Neural Network on MNIST\n",
        "train_loss_nnDD, train_acc_nnDD = modelDD.evaluate(X_train_img_scaled, y_train_img, verbose=0)\n",
        "test_loss_nnDD, test_acc_nnDD = modelDD.evaluate(X_test_img_scaled, y_test_img, verbose=0)\n",
        "print('Deeper NN with Digits dataset - Training Accuracy:', train_acc_nnDD, '| Test Accuracy:', test_acc_nnDD)"
      ],
      "metadata": {
        "id": "sYZ3oIdWy_5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approx. Expected output:**\n",
        "```\n",
        "Deeper NN with Digits dataset - Training Accuracy: 0.9843000173568726 | Test Accuracy: 0.9739000201225281\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mM34auEbvr5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Show the performance of our models in the MNIST fashion dataset\n",
        "\n",
        "Now use the same neural network architecture as before, but with a more complex dataset to observe how the performance changes.\n"
      ],
      "metadata": {
        "id": "UzwFSa8X0DGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load & Split  dataset\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#Scale dataset\n",
        "X_train = X_train_full/ 255.\n",
        "X_test = X_test / 255.\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Plot first 12 images\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(12):\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "    plt.imshow(X_train_full[i], cmap='gray')\n",
        "    plt.title(class_names[y_train_full[i]])\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gfmaeeIt0RFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets train out NN"
      ],
      "metadata": {
        "id": "6HeAHjPm1v_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build a simple feedforward neural network using Keras\n",
        "modelSF= Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Convert images to 1D vector\n",
        "    Dense(50, activation='relu'),   # Hidden layer\n",
        "    Dense(10, activation='softmax')  # Output layer for 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "modelSF.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model (using 10% of the training data as validation set)\n",
        "history = modelSF.fit(X_train, y_train_full, epochs=5, batch_size=32, validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "OLJYB1pX1xoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_loss_nnSF, train_acc_nnSF = modelSF.evaluate(X_train, y_train_full, verbose=0)\n",
        "test_loss_nnSF, test_acc_nnSF = modelSF.evaluate(X_test, y_test, verbose=0)\n",
        "print('Simple NN with Fashion dataset - Training Accuracy:', train_acc_nnSF, '| Test Accuracy:', test_acc_nnSF)"
      ],
      "metadata": {
        "id": "Dj0R29QZ1x_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build a simple feedforward neural network using Keras\n",
        "modelDF = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Convert images to 1D vector\n",
        "    Dense(400, activation='relu'),   # Hidden layer\n",
        "    Dense(300, activation='relu'),   # Hidden layer\n",
        "    Dense(200, activation='relu'),   # Hidden layer\n",
        "    Dense(10, activation='softmax')  # Output layer for 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "modelDF.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model (using 10% of the training data as validation set)\n",
        "history = modelDF.fit(X_train, y_train_full, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zFLmKcP02SFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_nnDF, train_acc_nnDF = modelDF.evaluate(X_train, y_train_full, verbose=0)\n",
        "test_loss_nnDF, test_acc_nnDF = modelDF.evaluate(X_test, y_test, verbose=0)\n",
        "print('Deeper NN with Fashion dataset - Training Accuracy:', train_acc_nnDF, '| Test Accuracy:', test_acc_nnDF)"
      ],
      "metadata": {
        "id": "uZIQ2adB3XZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lets compare our results"
      ],
      "metadata": {
        "id": "BzKkaqB2w3Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Simple NN with Digits dataset - Training Accuracy:', train_acc_nnSD, '| Test Accuracy:', test_acc_nnSD)\n",
        "print('Deeper NN with Digits dataset - Training Accuracy:', train_acc_nnDD, '| Test Accuracy:', test_acc_nnDD)\n",
        "\n",
        "print('Simple NN with Fashion dataset- Training Accuracy:', train_acc_nnSF, '| Test Accuracy:', test_acc_nnSF)\n",
        "print('Deeper NN with Fashion dataset - Training Accuracy:', train_acc_nnDF, '| Test Accuracy:', test_acc_nnDF)"
      ],
      "metadata": {
        "id": "FVgB2D1a2R7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Remember to explain the performance differences between the \"simpler\" and \"deeper\" neural network models, as well as between datasets**\n",
        "\n",
        "Observe that the \"deeper\" models tend to perform better than the \"simpler\" ones. Why might this be the case? Additionally, note that performance drops when using the same neural network architecture with a more complex dataset. Be sure to discuss the potential reasons behind these differences.\n"
      ],
      "metadata": {
        "id": "WTIW66s0kitP"
      }
    }
  ]
}