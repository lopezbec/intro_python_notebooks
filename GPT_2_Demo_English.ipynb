{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><b>© Content is made available under the CC-BY-NC-ND 4.0 license.\n",
        "\n",
        "---\n",
        "\n",
        "Christian Lopez, lopezbec@lafayette.edu<center>"
      ],
      "metadata": {
        "id": "pXP789oCH06g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook demonstrates the interactive generation of text using the GPT-2 Large model. You can enter a prompt and then generate text interactively, token by token, or specify the number of tokens to generate through a simple form. The notebook shows details such as tokenization and the main token predictions before generation.\n",
        "\n",
        "# About GPT-2 Large:\n",
        "## GPT-2 Large is one of the open-source language models developed by OpenAI, with 774 million parameters.\n"
      ],
      "metadata": {
        "id": "g5V7-YkXNMVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1) Installing some Python libraries and downloading the GPT-2 model\n",
        "# Install the transformers library (if not already installed)\n",
        "!pip install transformers\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Use GPT-2 Large as our model for English text generation\n",
        "model_name = \"gpt2-large\"  # GPT-2 Large has ~774M parameters\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Check if GPU is available and move the model to GPU if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GRmw2pgoHsxD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2) Here you can type your prompt, to see how it becomes a list of numbers\n",
        "# Get a prompt from the user\n",
        "prompt ='Lafayette College is located ' # @param {type:\"string\", placeholder:\"Enter a prompt\"}\n",
        "\n",
        "# Tokenize the prompt and print tokens and IDs\n",
        "tokenized_prompt = tokenizer.tokenize(prompt)\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "print(\"\\n--- Tokenization Details ---\")\n",
        "print(\"Tokenized prompt (tokens):\", tokenized_prompt)\n",
        "print(\"Tokenized prompt (token IDs):\", input_ids.tolist()[0])\n",
        "\n",
        "# Set temperature for randomness (lower value = more deterministic)\n",
        "temperature = 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "mruIJYc6H-8N",
        "outputId": "e2dbc20d-f76d-4f41-8e67-bedc7085bd1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tokenization Details ---\n",
            "Tokenized prompt (tokens): ['L', 'af', 'ayette', 'ĠCollege', 'Ġis', 'Ġlocated', 'Ġ']\n",
            "Tokenized prompt (token IDs): [43, 1878, 27067, 5535, 318, 5140, 220]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "orfP46_CjCAO",
        "outputId": "de93db32-9c62-4c9f-da5f-5124239e7ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top 5 Tokens for the Next Prediction ---\n",
            "Token: '', Probability: 1.0000\n",
            "Token: '________', Probability: 0.0000\n",
            "Token: '________________', Probability: 0.0000\n",
            "Token: '_____', Probability: 0.0000\n",
            "Token: 'Â', Probability: 0.0000\n",
            "\n",
            "--- Interactive Generation Process ---\n",
            "\n",
            "Step 1: Predicted token: '' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "Lafayette College is located  \n",
            "----------------------------------------\n",
            "Step 2: Predicted token: 'in' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "Lafayette College is located  in\n",
            "----------------------------------------\n",
            "Step 3: Predicted token: 'Lafayette' with probability: 0.9975\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "Lafayette College is located  in Lafayette\n",
            "----------------------------------------\n",
            "Step 4: Predicted token: ',' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "Lafayette College is located  in Lafayette,\n",
            "----------------------------------------\n",
            "Step 5: Predicted token: 'Louisiana' with probability: 0.9994\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "Lafayette College is located  in Lafayette, Louisiana\n",
            "----------------------------------------\n",
            "Step 6: Predicted token: '.' with probability: 0.9830\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "Lafayette College is located  in Lafayette, Louisiana.\n",
            "----------------------------------------\n",
            "Step 7: Predicted token: 'It' with probability: 0.6539\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "Lafayette College is located  in Lafayette, Louisiana. It\n",
            "----------------------------------------\n",
            "Step 8: Predicted token: 'is' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "Lafayette College is located  in Lafayette, Louisiana. It is\n",
            "----------------------------------------\n",
            "Step 9: Predicted token: 'a' with probability: 0.9999\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2463431516.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Ask the user if they want to continue generating the next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generate next token? (y/n): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# @title 3) This is to generate the prediction of the following words, one at a time\n",
        "\n",
        "\n",
        "\n",
        "# Compute logits for the current prompt\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Adjust logits with temperature for randomness and compute softmax probabilities\n",
        "next_token_logits = logits[:, -1, :] / temperature\n",
        "probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "# Get the top 5 tokens with the highest probabilities\n",
        "top5 = torch.topk(probs, 5)\n",
        "top5_ids = top5.indices[0]\n",
        "top5_probs = top5.values[0]\n",
        "top5_tokens = [tokenizer.decode(token_id) for token_id in top5_ids]\n",
        "\n",
        "print(\"\\n--- Top 5 Tokens for the Next Prediction ---\")\n",
        "for token, prob in zip(top5_tokens, top5_probs):\n",
        "    print(f\"Token: '{token.strip()}', Probability: {prob.item():.4f}\")\n",
        "\n",
        "# Initialize generated text with the prompt\n",
        "generated_text = prompt\n",
        "\n",
        "# Define the maximum number of tokens to generate in the interactive loop\n",
        "max_tokens = 50  # Adjust as needed\n",
        "\n",
        "print(\"\\n--- Interactive Generation Process ---\\n\")\n",
        "\n",
        "# Interactive loop: generate tokens one at a time based on user confirmation\n",
        "for i in range(max_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Adjust logits with temperature for randomness\n",
        "    next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    # Sample the next token from the probability distribution\n",
        "    next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "    next_token_prob = probs[0, next_token_id.item()]  # probability of the chosen token\n",
        "\n",
        "    # Decode the predicted token to a string\n",
        "    next_token = tokenizer.decode(next_token_id.squeeze())\n",
        "\n",
        "    # Append the predicted token to the input_ids for the next iteration\n",
        "    input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
        "    # Append the predicted token to the generated text\n",
        "    generated_text += next_token\n",
        "\n",
        "    # Print the predicted token and its probability\n",
        "    print(f\"Step {i+1}: Predicted token: '{next_token.strip()}' with probability: {next_token_prob.item():.4f}\")\n",
        "\n",
        "    # Ask the user if they want to continue generating the next token\n",
        "    user_input = input(\"Generate next token? (y/n): \")\n",
        "    if user_input.lower() not in [\"y\", \"yes\"]:\n",
        "        break\n",
        "\n",
        "    # Print the updated prompt/text after the user confirms\n",
        "    print(\"\\nUpdated text:\")\n",
        "    print(generated_text)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"\\n--- Full Generated Response ---\\n\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Here all at once\n",
        "\n",
        "Try changing the value of the `temperature` hyperparameter to be more \"creative\""
      ],
      "metadata": {
        "id": "NsUhOXazOqlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "# Get the initial prompt from the user\n",
        "prompt ='Lafayette College is located ' # @param {type:\"string\", placeholder:\"Enter a prompt\"}\n",
        "\n",
        "# Tokenize the prompt and encode to tensor\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "generated_text = prompt\n",
        "\n",
        "# Ask the user for the number of new tokens to generate\n",
        "num_tokens = 25 # @param {type:\"number\"}\n",
        "\n",
        "# Set temperature for randomness (lower value means more deterministic output)\n",
        "temperature = 0.1 # @param {type:\"slider\", min:0.1, max:0.9, step:0.1}\n",
        "\n",
        "print(\"\\n--- Generating Text ---\\n\")\n",
        "for i in range(num_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Adjust logits with temperature for randomness and compute probabilities\n",
        "    next_token_logits = logits[:, -1, :] / temperature\n",
        "    probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    # Sample the next token from the probability distribution\n",
        "    next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "    next_token = tokenizer.decode(next_token_id.squeeze())\n",
        "\n",
        "    # Append the predicted token to input_ids and generated text\n",
        "    input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
        "    generated_text += next_token\n",
        "\n",
        "    # Optionally, print each step\n",
        "    print(f\"Step {i+1}: Generated token: '{next_token.strip()}'\")\n",
        "\n",
        "print(\"\\n--- Final Generated Text ---\\n\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "7WtGjK01JlCr",
        "outputId": "ae774495-4a1b-4370-e625-1836256b58de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Text ---\n",
            "\n",
            "Step 1: Generated token: ''\n",
            "Step 2: Generated token: 'in'\n",
            "Step 3: Generated token: 'Lafayette'\n",
            "Step 4: Generated token: ','\n",
            "Step 5: Generated token: 'Louisiana'\n",
            "Step 6: Generated token: '.'\n",
            "Step 7: Generated token: 'It'\n",
            "Step 8: Generated token: 'is'\n",
            "Step 9: Generated token: 'a'\n",
            "Step 10: Generated token: 'private'\n",
            "Step 11: Generated token: 'liberal'\n",
            "Step 12: Generated token: 'arts'\n",
            "Step 13: Generated token: 'college'\n",
            "Step 14: Generated token: 'that'\n",
            "Step 15: Generated token: 'offers'\n",
            "Step 16: Generated token: 'a'\n",
            "Step 17: Generated token: 'variety'\n",
            "Step 18: Generated token: 'of'\n",
            "Step 19: Generated token: 'programs'\n",
            "Step 20: Generated token: 'including'\n",
            "Step 21: Generated token: 'a'\n",
            "Step 22: Generated token: 'Bachelor'\n",
            "Step 23: Generated token: 'of'\n",
            "Step 24: Generated token: 'Arts'\n",
            "Step 25: Generated token: 'in'\n",
            "\n",
            "--- Final Generated Text ---\n",
            "\n",
            "Lafayette College is located  in Lafayette, Louisiana. It is a private liberal arts college that offers a variety of programs including a Bachelor of Arts in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Resources\n",
        "\n",
        "If you are interested in learning more about the fundamentals of Large Language Models (LLMs) and how they work, here are some useful resources:\n",
        "\n",
        "### Websites\n",
        "- **[Hugging Face Transformers Documentation](https://huggingface.co/transformers/):**  \n",
        "  A complete guide on how to use transformer-based models, including tutorials and code examples.\n",
        "- **[OpenAI Blog](https://openai.com/blog/):**  \n",
        "  Read about the latest research, applications, and discussions around advanced language models.\n",
        "- **[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/):**  \n",
        "  A visual and intuitive explanation of transformer architectures and how they power modern language models.\n",
        "- **[Stanford CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/):**  \n",
        "  Explore lecture notes, slides, and video recordings on NLP and deep learning.\n",
        "\n",
        "### YouTube Channels and Videos\n",
        "- **Hugging Face – Transformers in Action:**  \n",
        "  Look for practical tutorials and demonstrations on using transformers on the [Hugging Face YouTube Channel](https://www.youtube.com/c/HuggingFace).\n",
        "- **Yannic Kilcher:**  \n",
        "  In-depth analysis of research papers and AI models, including explanations of GPT-2, GPT-3, and other LLMs. Visit his [YouTube channel](https://www.youtube.com/c/YannicKilcher).\n",
        "- **Two Minute Papers:**  \n",
        "  Short and accessible summaries of recent AI research, including advances in language models. Visit the [Two Minute Papers YouTube Channel](https://www.youtube.com/user/keeroyz).\n",
        "- **DeepLearning.AI:**  \n",
        "  Educational content and discussions about current trends in deep learning and AI. Watch videos on the [DeepLearning.AI YouTube Channel](https://www.youtube.com/c/Deeplearningai).\n",
        "\n",
        "These resources will help you deepen your understanding of LLMs and f\n"
      ],
      "metadata": {
        "id": "7EUGtx-3RWFi"
      }
    }
  ]
}