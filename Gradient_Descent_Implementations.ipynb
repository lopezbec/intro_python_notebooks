{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRkIEC7XmK4C"
      },
      "source": [
        "<center><b>© Content is made available under the CC-BY-NC-ND 4.0 license. Christian Lopez, lopezbec@lafayette.edu<center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62dHC1smMFB"
      },
      "source": [
        "![](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTHAMc6QmL5D"
      },
      "source": [
        "#Implementation of different Gradient Descent Algorithms\n",
        "In this notebook you will implement (i.e., train) a univariate Linear Regression model using:\n",
        "\n",
        "\n",
        "\n",
        "1.   Batch Gradient Descent\n",
        "2.   Stochastic Gradient Descent\n",
        "3.   Mini-batch Gradient Descent\n",
        "\n",
        "\n",
        "\n",
        "Most of the notebooks we are going to be using are inspired from existing notebooks that are available online and are made  free for educational purposes. Nonetheless, the notebooks of this class should not be share without prior permission of the instructor. When working in an assignment always remember the [Student Code of Conduct]( https://conduct.lafayette.edu/student-handbook/student-code-of-conduct/).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o65RqvnHtM1"
      },
      "source": [
        "###**Instructions:**\n",
        "- You will be using Python 3.\n",
        "\n",
        "- Only modify the code that is within the comments:\n",
        "\n",
        "`### START CODE HERE ###`\n",
        "\n",
        "`### END CODE HERE ###`\n",
        "\n",
        "- You need to run all the code cells on the notebok sequentially\n",
        "- If you are asked to change/update a cell, change/update and run it to check if your result is correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2sRosoUPC4P"
      },
      "source": [
        "\n",
        "Before we begin, we need to import all libraries required for this programming exercise and get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYAVdOzUPC4W"
      },
      "source": [
        "# used for manipulating directory paths\n",
        "import os\n",
        "\n",
        "# Scientific and vector computation for python\n",
        "import numpy as np\n",
        "\n",
        "# Plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n",
        "\n",
        "# tells matplotlib to embed plots within the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "#Pandas for summary statistics\n",
        "import pandas as pd\n",
        "\n",
        "#Scikit-learn for implemeting LinearRegression from a existing algorithm.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#To check how long our implementation takes\n",
        "import time\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/lopezbec/intro_python_notebooks/main/Data_LRIntro/ex1data1.txt\n",
        "!wget https://raw.githubusercontent.com/lopezbec/intro_python_notebooks/main/Data_LRIntro/ex1data2.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0rYDuKXPC5j"
      },
      "source": [
        "You will use the same truck data as in the \"Univariate_LR_implementation\" notebook. The file `Data/ex1data1.txt` contains the dataset for our linear regression problem. The first column is the population of a city (in 10,000s) and the second column is the profit of a food truck in that city (in $10,000s). A negative value for profit indicates a loss. \n",
        "\n",
        "We provide you with the code needed to load this data. The dataset is loaded from the data file into the variables `x` and `y`. We will also creare a matrix `X` with 1's in the 1st colums and the 2nd colums with the values of `x` for your vectorized implementation fo the algorithms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se3BUG2bPC5m"
      },
      "source": [
        "# Read comma separated data\n",
        "data = np.loadtxt('ex1data1.txt', delimiter=',')\n",
        "x, y = data[:, 0], data[:, 1]\n",
        "x=x.reshape(x.size,1)\n",
        "y=y.reshape(y.size,1)\n",
        "m = y.size  # number of training \n",
        "\n",
        "# Add a column of ones to X. The numpy function stack joins arrays along a given axis. \n",
        "# The first axis (axis=0) refers to rows (training examples) \n",
        "# and second axis (axis=1) refers to columns (features).\n",
        "X = np.stack([np.ones(m), x[:,0]], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03NIW6UWPC65"
      },
      "source": [
        "\n",
        "#### 0- Computing the cost $J(\\theta)$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mBmEYlPFifh"
      },
      "source": [
        "As you perform gradient descent to  minimize the cost function $J(\\theta)$,it is helpful to monitor the convergence by computing the cost. Now, we will implement a function to calculate $J(\\theta)$ so you can check the convergence of your gradient descent implementation. \n",
        "\n",
        "Your next task is to complete the code for the function `computeCost` which computes $J(\\theta)$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14wQXkanPC68"
      },
      "source": [
        "def computeCost(X, y, theta):\n",
        "    \"\"\"\n",
        "    Compute cost for linear regression. Computes the cost of using theta as the\n",
        "    parameter for linear regression to fit the data points in X and y.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The input dataset of shape (m x n), where m is the number of examples,\n",
        "        and n is the number of features. . We assume a vector of one's already \n",
        "        appended to the features, so in the  univariate case n=1+1 columns.\n",
        "    \n",
        "    y : array_like\n",
        "        The values of the function at each data point. This is a vector of\n",
        "        shape (m x 1).\n",
        "    \n",
        "    theta : array_like\n",
        "        The parameters for the regression function. This is a vector of \n",
        "        shape (2,1).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    J : float\n",
        "        The value of the regression cost function.\n",
        "    \n",
        "    Instructions\n",
        "    ------------\n",
        "    Compute the cost of a particular choice of theta. \n",
        "    You should set J to the cost.\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize some useful values\n",
        "    m = y.size  # number of training examples\n",
        "    \n",
        "    # You need to return the following variables correctly\n",
        "    J = 0\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "\n",
        "    h = X.dot(theta)\n",
        "    J = 1/(2*m)*np.sum(np.square(h-y))\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckwBoArQPC8v"
      },
      "source": [
        "\n",
        "#### 1- Batch Gradient descent\n",
        "\n",
        "Next, you will complete a function which implements Batch Gradient Descent.\n",
        "\n",
        "The starter code for the function `Batch_GD` calls `computeCost` on every iteration and saves the cost to a `python` list. Assuming you have implemented gradient descent and `computeCost` correctly, your value of $J(\\theta)$ should never increase, and should converge to a steady value by the end of the algorithm.\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySeYeNxEPC84"
      },
      "source": [
        "def Batch_GD(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs a vectorized gradient descent to learn `theta`. Updates theta by taking `num_iters`\n",
        "    gradient steps with learning rate `alpha`.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The input dataset of shape (m x n+1).\n",
        "    \n",
        "    y : arra_like\n",
        "        Value at given features. A vector of shape (m x 1 ).\n",
        "    \n",
        "    theta : array_like\n",
        "        Initial values for the linear regression parameters. \n",
        "        A vector of shape (n+1 x 1).\n",
        "    \n",
        "    alpha : float\n",
        "        The learning rate.\n",
        "    \n",
        "    num_iters : int\n",
        "        The number of iterations for gradient descent. \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    theta : array_like\n",
        "        The learned linear regression parameters. A vector of shape (n+1 x 1 ).\n",
        "    \n",
        "    J_history : list\n",
        "        A python list for the values of the cost function after each iteration.\n",
        "    \n",
        "    Instructions\n",
        "    ------------\n",
        "    Peform a single gradient step on the parameter vector theta.\n",
        "\n",
        "    While debugging, it can be useful to print out the values of \n",
        "    the cost function (computeCost) and gradient.\n",
        "    \"\"\"\n",
        "    # Initialize some useful values\n",
        "    m = y.shape[0]  # number of training examples\n",
        "    \n",
        "    # make a copy of theta, to avoid changing the original array, since numpy arrays\n",
        "    # are passed by reference to functions!!!\n",
        "    theta_cb = theta.copy()\n",
        "    \n",
        "    J_history = [] # Use a python list to save cost in every iteration, you would need to append a new element for every step\n",
        "\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "     # save the cost J in every iteration\n",
        "    \n",
        "    return theta_cb, J_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1QBTY7G8wVP"
      },
      "source": [
        "Now lets run your code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vrehGnIrCJN"
      },
      "source": [
        "# initialize fitting parameters\n",
        "theta = np.zeros(2).reshape(2,1)\n",
        "\n",
        "# some gradient descent settings\n",
        "iterations = 3000\n",
        "alpha = 0.02\n",
        "\n",
        "tic = time.process_time()\n",
        "theta, J_history_BGD = Batch_GD(X ,y, theta, alpha, iterations)\n",
        "toc = time.process_time()\n",
        "print('Theta found by Batch_GD after {0} iterations: {1}, {2}'.format(iterations,theta[0,0],theta[1,0] ))\n",
        "print(\"Cost value\" ,computeCost(X, y, theta))\n",
        "print(\"Time to run:\"+str(1000*(toc - tic)) + \"ms\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhS_JB5v84zN"
      },
      "source": [
        "**Expected output (approximately)**:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Theta found by Batch_GD after 3000 iterations:~ -3.89, 1.19\n",
        "Cost value 4.47\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8rBsUsK9PyE"
      },
      "source": [
        "Now lets plot $J(\\theta)$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grxxCNxD8a5h"
      },
      "source": [
        "# theta for minimized cost J\n",
        "plt.plot(J_history_BGD, \"b.\")\n",
        "plt.ylabel('Cost J')\n",
        "plt.ylim(4,16)\n",
        "plt.xlabel('Iterations');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgledrEqodEI"
      },
      "source": [
        "\n",
        "#### 2- Stochastic Gradient descent\n",
        "\n",
        "Next, you will complete a function which implements Stochastic Gradient Descent.\n",
        "\n",
        "The starter code for the function `Stochastic_GD` calls `computeCost` on every iteration and saves the cost to a `python` list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG94bJYRqLEP"
      },
      "source": [
        "\n",
        "def Stochastic_GD(X, y, theta,alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs a Stochastic gradient descent. Updates theta by taking `num_iters`\n",
        "    Stochastic gradient steps with learning rate `alpha`.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The input dataStochasticset of shape (m x n+1).\n",
        "    \n",
        "    y : arra_like\n",
        "        Value at given features. A vector of shape (m x 1 ).\n",
        "    \n",
        "    theta : array_like\n",
        "        Initial values for the linear regression parameters. \n",
        "        A vector of shape (n+1 x 1).\n",
        "    \n",
        "    alpha : float\n",
        "        The learning rate.\n",
        "    \n",
        "    num_iters : int\n",
        "        The number of iterations for gradient descent. \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    theta : array_like\n",
        "        The learned linear regression parameters. A vector of shape (n+1 x 1 ).\n",
        "    \n",
        "    J_history : list\n",
        "        A python list for the values of the cost function after each iteration.\n",
        "    \"\"\"\n",
        "    np.random.seed(23)\n",
        "   \n",
        "    # Initialize some useful values\n",
        "    m = y.shape[0]  # number of training examples\n",
        "    \n",
        "    # make a copy of theta, to avoid changing the original array, since numpy arrays\n",
        "    # are passed by reference to functions!!!\n",
        "    theta_cs = theta.copy()\n",
        "    J_history=[]\n",
        "    \n",
        "    for it in range(num_iters):\n",
        "        for i in range(m):\n",
        "    ### START CODE HERE ### (≈ 6 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###        \n",
        "            # This is to ensure we pick the best we have found so far, not just the last one\n",
        "        J_history.append(computeCost( X, y,theta_cs.reshape(2,1)))\n",
        "        if(min(J_history)>=computeCost( X, y,theta_cs.reshape(2,1))):\n",
        "            theta_min=theta_cs\n",
        "    return theta_min,J_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2HYR4ZQpCvZ"
      },
      "source": [
        "\n",
        "# initialize fitting parameters\n",
        "theta = np.zeros(shape=(2,1))\n",
        "\n",
        "# some gradient descent settings\n",
        "iterations = 100\n",
        "alpha=0.05\n",
        "tic = time.process_time()\n",
        "theta,J_history_SGD = Stochastic_GD(X ,y,theta,alpha, iterations)\n",
        "toc = time.process_time()\n",
        "print('Theta found by Stochastic_GD after {0} iterations: {1}, {2}'.format(iterations,theta[0,0],theta[1,0] ))\n",
        "print(\"Cost value\" ,computeCost(X, y, theta))\n",
        "print(\"Time to run:\"+str(1000*(toc - tic)) + \"ms\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wNY_vxC-nXX"
      },
      "source": [
        "**Expected output (approximately)**:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Theta found by Stochastic_GD after 100 iterations: -2.24, 1.02\n",
        "Cost value 4.72\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, given this dataset of just 97 samples (m) and 2 features (n), we cannot appreciate the speed of SGD, even after using a different alpha. The key thing to look is at the plot below, see the “jumping”"
      ],
      "metadata": {
        "id": "H2L0ggDr3DEV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnUlqRomntNp"
      },
      "source": [
        "Doing the `reshape` and finding the minimun value in the array is making this code run not as fast as ti should\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tNW4yRBspGW"
      },
      "source": [
        "# theta for minimized cost J\n",
        "plt.plot(J_history_SGD, \"b.\")\n",
        "# #plt.xlim(xmin=0.0)\n",
        "plt.ylabel('Cost J')\n",
        "plt.ylim(4,16)\n",
        "plt.xlabel('Iterations');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BShIDK57_FKR"
      },
      "source": [
        "\n",
        "#### 3- Mini-bacth Gradient descent\n",
        "\n",
        "Next, you will complete a function which implements Mini-bacth Gradient Descent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyQWNVlw_dgL"
      },
      "source": [
        "def Minibacth_GD(X, y, theta,alpha, num_iters,minibatch_size):\n",
        "    \"\"\"\n",
        "    Performs a Mini-bacth gradient descent. Updates theta by taking `num_iters`\n",
        "    gradient steps from a set sample siz =minibatch_size \n",
        "     with learning rate `alpha`.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The input dataStochasticset of shape (m x n+1).\n",
        "    \n",
        "    y : arra_like\n",
        "        Value at given features. A vector of shape (m x 1 ).\n",
        "    \n",
        "    theta : array_like\n",
        "        Initial values for the linear regression parameters. \n",
        "        A vector of shape (n+1 x 1).\n",
        "    \n",
        "    alpha : float\n",
        "        The learning rate.\n",
        "    \n",
        "    num_iters : int\n",
        "        The number of iterations for gradient descent.\n",
        "\n",
        "    minibatch_size: int \n",
        "         Size of mini-batch\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    theta : array_like\n",
        "        The learned linear regression parameters. A vector of shape (n+1 x 1 ).\n",
        "    \n",
        "    J_history : list\n",
        "        A python list for the values of the cost function after each iteration.\n",
        "    \"\"\"\n",
        "    np.random.seed(23)\n",
        "   \n",
        "    # Initialize some useful values\n",
        "    m = y.shape[0]  # number of training examples\n",
        "    \n",
        "    # make a copy of theta, to avoid changing the original array, since numpy arrays\n",
        "    # are passed by reference to functions!!!\n",
        "    theta_cm = theta.copy()\n",
        "    \n",
        "    J_history=[]\n",
        "    \n",
        "    for it in range(num_iters):\n",
        "        shuffled_indices = np.random.permutation(m)\n",
        "        X_shuffled = X[shuffled_indices]\n",
        "        y_shuffled = y[shuffled_indices]\n",
        "        for i in range(m):\n",
        "    ### START CODE HERE ### (≈ 7 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###        \n",
        "        J_history.append(computeCost( X, y,theta_cm.reshape(2,1)))\n",
        "        if(min(J_history)>=computeCost( X, y,theta_cm.reshape(2,1))):\n",
        "            theta_min=theta_cm\n",
        "    return theta_min,J_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBIaVOpqANRT"
      },
      "source": [
        "np.random.seed(12)\n",
        "# initialize fitting parameters\n",
        "theta = np.zeros(shape=(2,1))\n",
        "\n",
        "# some gradient descent settings\n",
        "iterations = 100\n",
        "alpha=0.1\n",
        "minibatch_size=30\n",
        "\n",
        "tic = time.process_time()\n",
        "theta,J_history = Minibacth_GD(X ,y,theta,alpha, iterations,minibatch_size)\n",
        "toc = time.process_time()\n",
        "print('Theta found by Minibacth_GDwith batch of {0} and after {1} iterations: {2}, {3}'.format(minibatch_size,iterations,theta[0,0],theta[1,0] ))\n",
        "print(\"Cost value\" ,computeCost(X, y, theta))\n",
        "print(\"Time to run:\"+str(1000*(toc - tic)) + \"ms\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7n8S2_QB_QV"
      },
      "source": [
        "**Expected output (approximately)**:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Theta found by Minibacth_GDwith batch of 30 and after 100 iterations: -3.01, 1.10\n",
        "Cost value 4.54\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06uyT_X2qijV"
      },
      "source": [
        "Doing the `reshape` and finding the minimun value in the array is making this codes run not as fast as it should"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFOFlAKSBS5q"
      },
      "source": [
        "# theta for minimized cost J\n",
        "plt.plot(J_history, \"b.\")\n",
        "plt.ylim(4,16)\n",
        "plt.ylabel('Cost J')\n",
        "plt.xlabel('Iterations');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-NtyA7qB0EW"
      },
      "source": [
        "###Now let's visualize our results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4LzgGPAEW2r"
      },
      "source": [
        "xx = np.arange(5,24)\n",
        "theta = np.zeros(2).reshape(2,1)\n",
        "\n",
        "#Batch_GD\n",
        "iterations = 3000\n",
        "alpha = 0.01\n",
        "theta_Batch_GD, J_history = Batch_GD(X ,y, theta, alpha, iterations)\n",
        "yy_Batch_GD = theta_Batch_GD[0]+theta_Batch_GD[1]*xx\n",
        "\n",
        "#Stochastic_GD\n",
        "iterations = 100\n",
        "alpha=0.01\n",
        "theta_Stochastic_GD,J_history = Stochastic_GD(X ,y,theta,alpha, iterations)\n",
        "yy_Stochastic_GD = theta_Stochastic_GD[0]+theta_Stochastic_GD[1]*xx\n",
        "\n",
        "\n",
        "#Minibacth_GD\n",
        "iterations = 100\n",
        "alpha=0.01\n",
        "minibatch_size=30\n",
        "theta_Minibacth_GD,J_history = Minibacth_GD(X ,y,theta,alpha, iterations,minibatch_size)\n",
        "yy_Minibacth_GD = theta_Minibacth_GD[0]+theta_Minibacth_GD[1]*xx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPbfOMlS4QXH"
      },
      "source": [
        "# Plot gradient descent\n",
        "\n",
        "plt.scatter(X[:,1], y, s=30, c='c', marker='o', linewidths=1)\n",
        "plt.plot(xx,yy_Batch_GD, label='Batch_GD')\n",
        "plt.plot(xx,yy_Stochastic_GD, label='Stochastic_GD')\n",
        "plt.plot(xx,yy_Minibacth_GD, label='Minibacth_GD')\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "\n",
        "plt.xlim(5,22.5)\n",
        "plt.xlabel('Population of City in 10,000s')\n",
        "plt.ylabel('Profit in $10,000s')\n",
        "plt.legend(loc=4);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWWdNlJ2DPVs"
      },
      "source": [
        "## Training on a Synthetic dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuBszjJhDYyw"
      },
      "source": [
        "Lets create our own dataset with m=1,000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESjgV-YzDrP-"
      },
      "source": [
        "m=1000\n",
        "np.random.seed(12)\n",
        "x_synthetic = 20 * np.random.rand(m,1)\n",
        "y_synthetic = 5 + 5* x_synthetic+ x_synthetic**3 + 2*m*np.random.randn(m, 1)\n",
        "plt.plot(x_synthetic, y_synthetic, 'ob', ms=5)\n",
        "plt.ylabel('y')\n",
        "plt.xlabel('x')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byTeN4UjH1uS"
      },
      "source": [
        "Lets implement our algorithms (this might take a few minutes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ws8Po3EmzZ"
      },
      "source": [
        "X_synthetic = np.stack([np.ones(m), x_synthetic[:,0]], axis=1)\n",
        "np.random.seed(12)\n",
        "xx = np.arange(0,24)\n",
        "theta = np.zeros(2).reshape(2,1)\n",
        "\n",
        "#Batch_GD\n",
        "iterations = 3000\n",
        "alpha = 0.01\n",
        "tic = time.process_time()\n",
        "theta_Batch_GD, J_history = Batch_GD(X_synthetic ,y_synthetic, theta, alpha, iterations)\n",
        "toc = time.process_time()\n",
        "Batch_GD_time=(1000*(toc - tic))\n",
        "yy_Batch_GD = theta_Batch_GD[0]+theta_Batch_GD[1]*xx\n",
        "Batch_GD_cost=computeCost(X_synthetic, y_synthetic, theta_Batch_GD)\n",
        "\n",
        "\n",
        "#Stochastic_GD\n",
        "iterations = 1\n",
        "alpha=0.3\n",
        "tic = time.process_time()\n",
        "theta_Stochastic_GD,J_history = Stochastic_GD(X_synthetic ,y_synthetic,theta,alpha, iterations)\n",
        "toc = time.process_time()\n",
        "Stochastic_GD_time=(1000*(toc - tic))\n",
        "yy_Stochastic_GD = theta_Stochastic_GD[0]+theta_Stochastic_GD[1]*xx\n",
        "Stochastic_GD_cost=computeCost(X_synthetic, y_synthetic, theta_Stochastic_GD)\n",
        "\n",
        "#Minibacth_GD\n",
        "iterations = 1\n",
        "alpha=0.1\n",
        "minibatch_size=20\n",
        "tic = time.process_time()\n",
        "theta_Minibacth_GD,J_history = Minibacth_GD(X_synthetic ,y_synthetic,theta,alpha, iterations,minibatch_size)\n",
        "toc = time.process_time()\n",
        "Minibacth_GD_time=(1000*(toc - tic))\n",
        "yy_Minibacth_GD = theta_Minibacth_GD[0]+theta_Minibacth_GD[1]*xx\n",
        "Minibacth_GD_cost=computeCost(X_synthetic, y_synthetic, theta_Minibacth_GD)\n",
        "\n",
        "# Plot gradient descent\n",
        "\n",
        "plt.scatter(X_synthetic[:,1], y_synthetic, s=30, c='c', marker='o', linewidths=0.5)\n",
        "plt.plot(xx,yy_Batch_GD, label='Batch_GD: Time={}ms, Cost={}'.format(round(Batch_GD_time),round(Batch_GD_cost,0)))\n",
        "plt.plot(xx,yy_Stochastic_GD, label='Stochastic_GD: Time={}ms, Cost={}'.format(round(Stochastic_GD_time),round(Stochastic_GD_cost,0)))\n",
        "plt.plot(xx,yy_Minibacth_GD, label='Minibacth_GD: Time={}ms, Cost={}'.format(round(Minibacth_GD_time),round(Minibacth_GD_cost,0)))\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "plt.xlim(0,20)\n",
        "plt.xlabel('Population of City in 10,000s')\n",
        "plt.ylabel('Profit in $10,000s')\n",
        "plt.legend(loc=4);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected output (approximately)**:\n",
        "\n",
        "```\n",
        "Batch_GD:       2391785\n",
        "Stochastic_GD:  2736806\n",
        "Minibatch_GD:   2766915\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "QRhQspjeCbCi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr5d-t8sX4bD"
      },
      "source": [
        "**DO NOT DELETE NOR MODIFY THESE CODE CELLS**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KfVgysmpGhLT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}