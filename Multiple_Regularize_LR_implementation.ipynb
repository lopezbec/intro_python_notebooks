{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvCaYYygG0ak"
      },
      "source": [
        "<center><b>© Content is made available under the CC-BY-NC-ND 4.0 license. Christian Lopez, lopezbec@lafayette.edu<center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/lopezbec/intro_python_notebooks/blob/master/Multiple_Regularize_LR_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td>"
      ],
      "metadata": {
        "id": "zBLg75YHzFwp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWckiY_RphgE"
      },
      "source": [
        "#Multivariate and Regularized Linear Regression Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI-kbu-zojyb"
      },
      "source": [
        "\n",
        "Most of the notebooks we are going to be using are inspired from existing notebooks that are available online and are made  free for educational purposes. Nonetheless, the notebooks of this class should not be share without prior permission of the instructor. When working in an assignment always remember the [Student Code of Conduct]( https://conduct.lafayette.edu/student-handbook/student-code-of-conduct/).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Ms9NtuUHwi"
      },
      "source": [
        "###**Instructions:**\n",
        "- You will be using Python.\n",
        "\n",
        "- Only modify the code that is within the comments:\n",
        "\n",
        "`### START CODE HERE ###`\n",
        "\n",
        "`### END CODE HERE ###`\n",
        "\n",
        "- You need to run all the code cells on the notebok sequentially\n",
        "- If you are asked to change/update a cell, change/update and run it to check if your result is correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QwasjN6GMgL"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFsdePPeGMgk"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "#Scikit-learn for implemeting LinearRegression from a existing algorithm.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def computeCost(X, y, theta):\n",
        "    return 1/(2*y.size)*np.sum(np.square(X.dot(theta)-y))\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/lopezbec/intro_python_notebooks/main/Data_MRLR/House_data_multiv.txt\n",
        "!wget https://raw.githubusercontent.com/lopezbec/intro_python_notebooks/main/Data_MRLR/FremontBridge.csv\n",
        "!wget https://raw.githubusercontent.com/lopezbec/intro_python_notebooks/main/Data_MRLR/BicycleWeatherdata.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTmW_k3XaWDa"
      },
      "source": [
        "# 1- Linear regression with multiple variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_aqzSLHRSnG"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
        "\n",
        "The file `House_data_multiv.txt` contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price\n",
        "of the house. \n",
        "\n",
        "<a id=\"section4\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pGfrvnCaU51"
      },
      "source": [
        "### 1.2- Feature Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhIGoitRaArf"
      },
      "source": [
        "\n",
        "\n",
        "We start by loading and displaying some values from this dataset. By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTJwr7N2RSnI"
      },
      "source": [
        "# Load data\n",
        "data = np.loadtxt( 'House_data_multiv.txt', delimiter=',')\n",
        "X = data[:, :2]\n",
        "y = data[:, 2]\n",
        "y=y[:,np.newaxis]\n",
        "\n",
        "m = y.size\n",
        "# print out some data points\n",
        "print('{:>8s}{:>8s}{:>10s}'.format('X[:,0]', 'X[:, 1]', 'y'))\n",
        "print('-'*26)\n",
        "for i in range(10):\n",
        "    print('{:8.0f}{:8.0f}{:10.0f}'.format(X[i, 0], X[i, 1], y[i,0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KICCJlDtRSnN"
      },
      "source": [
        "Your task here is to complete the code in `feature_Normalize_implementation` function:\n",
        "- Subtract the mean value of each feature from the dataset.\n",
        "- After subtracting the mean, additionally scale (divide) the feature values by their respective “standard deviations.”\n",
        "\n",
        "The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within ±2 standard deviations of the mean); this is an alternative to taking the range of values (max-min). In `numpy`, you can use the `std` function to compute the standard deviation. \n",
        "\n",
        "For example, the quantity `X[:, 0]` contains all the values of $x_1$ (house sizes) in the training set, so `np.std(X[:, 0])` computes the standard deviation of the house sizes.\n",
        "At the time that the function `feature_Normalize_implementation` is called, the extra column of 1’s corresponding to $x_0 = 1$ has not yet been added to $X$. \n",
        "\n",
        "You will do this for all the features and your code should work with datasets of any sizes (any number of features / examples). Note that each column of the matrix $X$ corresponds to one feature.\n",
        "\n",
        "**Implementation Note:** When normalizing the features, it is important\n",
        "to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters\n",
        "from the model, we often want to predict the prices of houses we have not\n",
        "seen before. Given a new x value (living room area and number of bedrooms), we must first normalize x using the mean and standard deviation that we had previously computed from the training set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZh6B10lakc-"
      },
      "source": [
        "#### 1.2  Excersice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAEuXUM5RSnQ"
      },
      "source": [
        "def  feature_Normalize_implementation(X):\n",
        "    \"\"\"\n",
        "    Normalizes the features in X. returns a normalized version of X where\n",
        "    the mean value of each feature is 0 and the standard deviation\n",
        "    is 1. This is often a good preprocessing step to do when working with\n",
        "    learning algorithms.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The dataset of shape (m x n).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    X_norm : array_like\n",
        "        The normalized dataset of shape (m x n).\n",
        "    \"\"\"\n",
        "    # You need to set these values correctly\n",
        "    X_norm = X.copy()\n",
        "    mu = np.zeros(X.shape[1])\n",
        "    sigma = np.zeros(X.shape[1])\n",
        "\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X_norm, mu, sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRuSXTXJeow9"
      },
      "source": [
        "Lets check your code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cvW4XeWeogy"
      },
      "source": [
        "X_norm, mu, sigma= feature_Normalize_implementation(X)\n",
        "\n",
        "print('{:>8s}{:>15s}'.format('X_norm[:,0]', 'X_norm[:, 1]'))\n",
        "print('-'*26)\n",
        "for i in range(5):\n",
        "    print('{:>8.3f}{:>15.3f}'.format(X_norm[i, 0], X_norm[i, 1]))\n",
        "\n",
        "print(\"Features means:{:>9.3f}{:>14.3f}\".format(mu[0], mu[1]))\n",
        "print(\"Features Std:{:>10.3f}{:>15.3f}\".format(sigma[0], sigma[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4rROBq77HlT"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "\n",
        "```\n",
        "X_norm[:,0]   X_norm[:, 1]\n",
        "--------------------------\n",
        "   0.131         -0.226\n",
        "  -0.510         -0.226\n",
        "   0.508         -0.226\n",
        "  -0.744         -1.554\n",
        "   1.271          1.102\n",
        "Features means: 2000.681        3.170\n",
        "Features Std:   786.203         0.753\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ug-OPE37wP2"
      },
      "source": [
        "We can alos normalized our data using the `StandardScaler()` from  [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXFt77Pp7wll"
      },
      "source": [
        "#Get the Mean and SD used to normalize the data, so we can apply it to our testing\n",
        "std_scaler = StandardScaler()\n",
        "X_norm_sklearn=std_scaler.fit_transform(X)\n",
        "mu_sklearn=std_scaler.fit(X).mean_\n",
        "sigma_sklearn=np.sqrt(std_scaler.fit(X).var_)\n",
        "\n",
        "print('{:>8s}{:>15s}'.format('X_norm_sklearn[:,0]', 'X_norm_sklearn[:, 1]'))\n",
        "print('-'*26)\n",
        "for i in range(5):\n",
        "    print('{:>8.3f}{:>15.3f}'.format(X_norm_sklearn[i, 0], X_norm_sklearn[i, 1]))\n",
        "\n",
        "print(\"Features means:{:>9.3f}{:>13.3f}\".format(mu_sklearn[0],mu_sklearn[1]))\n",
        "print(\"Features Std:{:>10.3f}{:>14.3f}\".format(sigma_sklearn[0], sigma_sklearn[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFP9Prfp6qMl"
      },
      "source": [
        "**Sanity check:**\n",
        "\n",
        "If we get the mean and std of the normalized features we should have means approx. 1, stds approx. 0. If you look at the actual values without the rounding (i.e., change the `3` for `24` on the code `{:>12.3f}`) you will see it is not exactly 1 nor 0 due to computational error)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWx06UMd6rvH"
      },
      "source": [
        "X_norm2, mu2, sigma2= feature_Normalize_implementation(X_norm)\n",
        "print(\"Normalized features means:{:>8.3f}{:>12.3f}\".format(mu2[0], mu2[1]))\n",
        "print(\"Normalized features Std:{:>9.3f}{:>13.3f}\".format(sigma2[0], sigma2[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scFQvKE27Umi"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "```\n",
        "Normalized features means:  -0.000       0.000\n",
        "Normalized features Std:    1.000        1.000\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npkyArm7Ys5Y"
      },
      "source": [
        "### 1.3 - Ridge Regression with the Normal Equation \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz7DUgS0YsKe"
      },
      "source": [
        "\n",
        "Thr closed-form solution to Ridge Linear Regression is:\n",
        "\n",
        "$$ \\theta = \\left( X^T X + \\lambda A\\right)^{-1} X^T{y}$$\n",
        "\n",
        "Where $A$ is the $(n +1)\\times(n+1)$ identity matrix, except with a 0 in the top-left cell, corresponting tot he bias/intercept term. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYK14pMHALR7"
      },
      "source": [
        "#### 1.3 **Excersice**\n",
        "\n",
        "Now you need to complete the `normalEqn_Reg` function to performe the closed-form solution to Ridge Linear Regression for any matrix $X$ and regularization term $\\lambda$\n",
        "\n",
        "Hints: Look at `np.identity` & `np.linalg.inv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqVRoEPxYr4G"
      },
      "source": [
        "def normalEqn_Reg(X, y,lambda_term):\n",
        "    \"\"\"\n",
        "    Computes the closed-form solution to linear regression using the normal equations.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The dataset of shape (m x n+1).\n",
        "    \n",
        "    y : array_like\n",
        "        The value at each data point. A vector of shape (m x 1).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    theta : array_like\n",
        "        Estimated linear regression parameters. A vector of shape (n+1 x 1).\n",
        "    \n",
        "    Instructions\n",
        "    ------------\n",
        "    Complete the code to compute the closed form solution to linear\n",
        "    regression and put the result in theta.\n",
        "    \n",
        "    Hint\n",
        "    ----\n",
        "    Look up the function `np.linalg.pinv` for computing matrix inverse.\n",
        "    \"\"\"\n",
        "    theta = np.zeros(X.shape[1])\n",
        "\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return theta.reshape(X.shape[1],1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eqnjnLQ_1m8"
      },
      "source": [
        "Lets test your solution. To implement Ridge Linear Regression is key to Normalized the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTYWiEI2ZLzC"
      },
      "source": [
        "#Let use the same data as for our \"Day9_Regularized_Linear_Regression.ipynb\" notebook\n",
        "np.random.seed(42)\n",
        "m = 20\n",
        "X = 3 * np.random.rand(m, 1)\n",
        "y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfxApT-sYrfc",
        "cellView": "both"
      },
      "source": [
        "#Create polynomial features\n",
        "Degree_of_the_Polynomial_Model=5\n",
        "\n",
        "#\"include_bias=False\" since we dont want to normalized the intercep/bias term\n",
        "poly_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "#Get the Mean and SD used to normalize the data, so we can apply it to our testing\n",
        "std_scaler = StandardScaler()\n",
        "X_scale=std_scaler.fit(X_poly)\n",
        "\n",
        "#Scaling our data is key for Regularized LR to work\n",
        "X_poly=std_scaler.fit_transform(X_poly)\n",
        "\n",
        "#If you comment the line above you will see gradient descent will not work well (or at all)\n",
        "X_poly = np.hstack([np.ones(shape=(y.size,1)), X_poly])\n",
        "\n",
        "# initialize fitting parameters (n+1)\n",
        "theta= np.zeros(Degree_of_the_Polynomial_Model+1).reshape(Degree_of_the_Polynomial_Model+1,1)\n",
        "\n",
        "\n",
        "alpha = 0.1       #0.3\n",
        "lambda_term=0.02\n",
        "\n",
        "\n",
        "theta_best= normalEqn_Reg(X_poly,y,lambda_term)\n",
        "print(theta_best)\n",
        "theta_cost=computeCost(X_poly, y, theta_best)\n",
        "print(\"Cost value= {}\".format(round(theta_cost,4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSx0zqnqYrMf"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "For Degrees=5, lambda_term=0.02\n",
        "```\n",
        "[[ 1.50467735]\n",
        " [ 1.02273468]\n",
        " [-1.59103516]\n",
        " [-0.06054524]\n",
        " [ 0.47300252]\n",
        " [ 0.60908347]]\n",
        "Cost value= 0.1431\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XrkRzdyDu38"
      },
      "source": [
        "#### 1.3.1 - Ridge Regression with scikit-learn "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT9F8M2mL_Cg"
      },
      "source": [
        "Here is the code on how to run the same Ridge Linear Regression using scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXsxT3T9NMkN"
      },
      "source": [
        "#Import Ridge Linear Regression\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "#Set the hyperparameter\n",
        "ridge_reg = Ridge(alpha=lambda_term)\n",
        "\n",
        "#Lets pass out X matrix with the polynomial terms\n",
        "ridge_reg.fit(X_poly, y)\n",
        "\n",
        "#Lets get the thetas\n",
        "coe=ridge_reg.coef_\n",
        "interc=ridge_reg.intercept_\n",
        "coe=coe.reshape(coe.size,1)\n",
        "interc=interc.reshape(interc.size,1)\n",
        "theta_sklearn=np.vstack([interc,coe[1:, :]])\n",
        "print(theta_sklearn)\n",
        "theta_sklearn_cost=computeCost(X_poly, y, theta_sklearn)\n",
        "print(\"Cost value= {}\".format(round(theta_sklearn_cost,4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-28hwQfr-aAy"
      },
      "source": [
        "### 1.4. Let’s plot our Regularized Linear Regression model.\n",
        "\n",
        "Pay attention on how we need to use the original mean and std used to transform the training set, to now transform our inputs for our predictions (the x to create our line)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMIQ_sOPaaVV"
      },
      "source": [
        "#Lets create some new data to create a line\n",
        "X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
        "#Add some polynomial term to this data\n",
        "poly_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\n",
        "X_new_poly = poly_features.fit_transform(X_new)\n",
        "\n",
        "#WE NEED TO SCALE OUR DATA BASED ON THE SCALE WE USE FOR TRAINING!!!!\n",
        "X_new_poly_sclae=X_scale.transform(X_new_poly)\n",
        "\n",
        "#Add Theta_0\n",
        "X_new_poly_sclae_0 = np.hstack([np.ones(shape=(X_new.size,1)), X_new_poly_sclae])\n",
        "\n",
        "#Lets cacuate our model (h, y_hat)\n",
        "y_newbig_theta_best= X_new_poly_sclae_0.dot(theta_best)\n",
        "\n",
        "#Plot models\n",
        "plt.plot(X_new, y_newbig_theta_best, \"g\", linewidth=1,  label='Normal Eq. Implementation')\n",
        "\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.xlabel(\"$x$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=4);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZo-UaroDWzk"
      },
      "source": [
        "### 1.5. MAE, MSE, RMSE, $R^2$, and adjusted $R^2$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_bm_-VTEX7k"
      },
      "source": [
        "It is quite common for the cost function used during training to be\n",
        "different from the performance measure used for testing (we will talk even more about this later). Apart from regularization, another reason they might be different is that a\n",
        "good training cost function should have optimization-friendly\n",
        "derivatives, while the performance measure used for testing should\n",
        "be as close as possible to the final objective we want to achive. \n",
        "\n",
        "For example, regression model are often trained using a cost function MSE but evaluated using RMSE or $R^2$. Similarly, classifiers are often trained using a cost function such as the log loss but evaluated using precision/recall or accuracy.\n",
        "\n",
        "So what are the different metric to evaluate a regression model?\n",
        "\n",
        "The MAE, MSE, RMSE, and $R^2$ metrics are mainly used to evaluate the prediction error rates and model performance of regression models.\n",
        "<br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EWmI3Ph2RAR"
      },
      "source": [
        "#### 1.5.1- **MAE** (Mean absolute error)\n",
        "\n",
        "Represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
        "\n",
        "\n",
        "$$ MAE = \\frac{1}{m}\\sum_{i=1}^m |\\ h_{\\theta}(x^{i})- y^{i}|$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1htE2X9mK0OT"
      },
      "source": [
        "##### **Excersice MAE**\n",
        "\n",
        "Now you need to complete the `MAE` function to calculate the Mean absolute error. You need to complete the docstring information missing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wkKAOSlEXHD"
      },
      "source": [
        "def MAE(y_true,y_pred):\n",
        "    \"\"\"\n",
        "    NEED TO COMPLETE\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array_like\n",
        "        The value at each data point. A vector of shape (m x 1).\n",
        "\n",
        "    y_pred : array_like\n",
        "        The predicted data values. A vector of shape (m x 1).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      NEED TO COMPLETE\n",
        "    \n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        "   \n",
        "    ### END CODE HERE ###\n",
        "    return MAE_val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj9LY1AG14I_"
      },
      "source": [
        "Lets test you code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJfa7g7eMgV1"
      },
      "source": [
        "y_pre=ridge_reg.predict(X_poly)\n",
        "MAE_val=MAE(y,y_pre)\n",
        "print(\"MAE=\", round(MAE_val,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qFtpVJL2A3v"
      },
      "source": [
        "We can also use the sklearn metric to calculate the MAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0XeBs-XOnOf"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "MAE_skl=mean_absolute_error(y, y_pre)\n",
        "print(\"sklearn MAE =\", round(MAE_skl,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ9xLeYl2csG"
      },
      "source": [
        "#### 1.5.2- **MSE** (Mean Squared Error) \n",
        "\n",
        "Represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
        "\n",
        "$$ MSE = \\frac{1}{m}\\sum_{i=1}^m\\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjxSv4w91qwB"
      },
      "source": [
        "##### **Excersice MSE**\n",
        "\n",
        "Now you need to complete the `MSE`function to calculate the Mean Squared Error. You also need to complete the docstring information missing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DODIQ9s_O7d1"
      },
      "source": [
        "def MSE(y_true,y_pred):\n",
        "    \"\"\"\n",
        "    NEED TO COMPLETE\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array_like\n",
        "        The value at each data point. A vector of shape (m x 1).\n",
        "\n",
        "    y_pred : array_like\n",
        "        The predicted data values. A vector of shape (m x 1).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      NEED TO COMPLETE\n",
        "    \n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        "   \n",
        "    ### END CODE HERE ###\n",
        "    return MSE_val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L3AOjpA3KkA"
      },
      "source": [
        "Lets test you code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B0dwdx3PO34"
      },
      "source": [
        "MSE_val=MSE(y,y_pre)\n",
        "print(\"MSE=\", round(MSE_val,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFjnM7NM3Ou5"
      },
      "source": [
        "We can also use the sklearn metric to calculate the MSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41RLL7pNPxBd"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "MSE_skl=mean_squared_error(y, y_pre)\n",
        "print(\"sklearn MSE =\", round(MSE_skl,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBbnR8An2mUb"
      },
      "source": [
        "#### 1.5.3- **RMSE**(Root Mean Squared Error)\n",
        "\n",
        "\n",
        "Is the error rate by the square root of MSE.\n",
        "\n",
        "$$ RMSE = \\sqrt{MSE}= \\sqrt{ \\frac{1}{m}\\sum_{i=1}^m\\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYQPEeId3eZq"
      },
      "source": [
        "##### **Excersice RMSE**\n",
        "\n",
        "Now you need to complete the `RMSE`function to calculate the Root Mean Squared Error. You also need to complete the docstring information missing and cannot call the MSE function within the RMSE function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRKrVw3gQq3_"
      },
      "source": [
        "def RMSE(y_true,y_pred):\n",
        "    \"\"\"\n",
        "    NEED TO COMPLETE\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array_like\n",
        "        The value at each data point. A vector of shape (m x 1).\n",
        "\n",
        "    y_pred : array_like\n",
        "        The predicted data values. A vector of shape (m x 1).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      NEED TO COMPLETE\n",
        "    \n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return RMSE_val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZqp7uiT3o-y"
      },
      "source": [
        "Lets test you code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKPNsPTmQ52h"
      },
      "source": [
        "RMSE_val=RMSE(y,y_pre)\n",
        "print(\"RMSE=\", round(RMSE_val,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBQPjNfq3q88"
      },
      "source": [
        "We can also use the sklearn metric to calculate the RMSE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH-hhpcORS-s"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "RMSE_skl=sqrt(mean_squared_error(y,y_pre))\n",
        "print(\"sklearn RMSE =\", round(RMSE_skl,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekxG2Oip2m-Q"
      },
      "source": [
        "#### 1.5.4- **$R^2$** (Coefficient of determination)\n",
        "\n",
        "\n",
        "\n",
        "Represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages (i.e., percentage of the data variability explained by the model). The higher the value is, the better the model is.\n",
        "\n",
        "$$ R^2 = 1- \\frac{\\sum_{i=1}^m\\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2}{\\sum_{i=1}^m\\left( \\bar{y} - y^{(i)}\\right)^2}$$\n",
        "\n",
        "Where $\\bar{y}$ is the mean value of $y$\n",
        "<br></br>\n",
        "\n",
        "**$R^2$-adjusted** is a modified version of $R^2$ that has been adjusted for the number of predictors in the model. The adjusted $R^2$  increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The adjusted$R^2$  can be negative, but it’s usually not.  It is always lower than the $R^2$\n",
        "\n",
        "$$ R^2-adj= 1-(1-R^2)\\left[\\frac{m-1}{m-(n+1)}\\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-vaDvzJ6kxk"
      },
      "source": [
        "##### **Excersice $R^2$**\n",
        "\n",
        "Now you need to complete the `R_square`function to calculate the Coefficient of determination. You also need to complete the docstring information missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csN5zgwlSY6S"
      },
      "source": [
        "def R_square(y_true,y_pred):\n",
        "    \"\"\"\n",
        "    NEED TO COMPLETE\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array_like\n",
        "        The value at each data point. A vector of shape (m x 1).\n",
        "\n",
        "    y_pred : array_like\n",
        "        The predicted data values. A vector of shape (m x 1).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      NEED TO COMPLETE\n",
        "    \n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return R_square_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8W0YAlW4GPY"
      },
      "source": [
        "Lets test you code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otU-D8crp5EX"
      },
      "source": [
        "R_square_val=R_square(y,y_pre)\n",
        "print(\"R_square=\", round(R_square_val,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2dnVy3D4LJi"
      },
      "source": [
        "We can also use the sklearn metric to calculate the $R^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxjID-5lqVAf"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "R_square_skl=r2_score(y, y_pre)\n",
        "print(\"sklearn R_square =\", round(R_square_skl,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "scSY7J0jR22M"
      },
      "source": [
        "# 2- Using Linear Regression for Predicting Bicycle Traffic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "UE1QvyFcR22P"
      },
      "source": [
        "As an practical example, we will look at whether we can predict the number of bicycle trips across Seattle's Fremont Bridge based on weather, season, and other factors.\n",
        "\n",
        "\n",
        "We will perform some basic feature engineering, and will join the bike data with another dataset, and try to determine the extent to which weather and seasonal factors (e.g., temperature, precipitation, and daylight hours) affect the volume of bicycle traffic.\n",
        "\n",
        "Fortunately, the NOAA makes available their daily [weather station data](http://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND) (We will used station ID [USW00024233](https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00024233/detail)) and we can easily use the [Pandas library](https://pandas.pydata.org/) to join the two data sources.\n",
        "\n",
        "We will perform a multivariate linear regression and a Ridge Linear Regression  to map features (i.e., weather and other information) to bicycle counts. We wil also look at the model's parameter to estimate how a change in any one of these parameters affects the number of riders on a given day.\n",
        "\n",
        "Let's start by loading the two datasets, indexing by date:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hT3RGnmc40KC"
      },
      "source": [
        "#!curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "t12toiu3R22g"
      },
      "source": [
        "#Data is already on your data folder\n",
        "counts = pd.read_csv('FremontBridge.csv', index_col='Date', parse_dates=True)\n",
        "weather = pd.read_csv('BicycleWeatherdata.csv', index_col='DATE', parse_dates=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6L3ZGyYtR22p"
      },
      "source": [
        "Next we will compute the total daily bicycle traffic, and put this in its own dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "V48W3tVER22t"
      },
      "source": [
        "daily = counts.resample('d').sum()\n",
        "daily['Total'] = daily.sum(axis=1)\n",
        "daily = daily[['Total']] # remove other columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4jxBcqoNR223"
      },
      "source": [
        "Bicycle use could vary from day to day; so let's account for this in our data by adding binary columns that indicate the day of the week. While the feature of \"day of the week\" is a categorical varaible, we can create a sereis of binary \"dummy\" variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UDvzvDXKR227"
      },
      "source": [
        "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "for i in range(7):\n",
        "    daily[days[i]] = (daily.index.dayofweek == i).astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6VxjUd5jR23E"
      },
      "source": [
        "Similarly, we might expect riders to behave differently on holidays; let's add an indicator of this as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3TwzgFM-R23H"
      },
      "source": [
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "cal = USFederalHolidayCalendar()\n",
        "holidays = cal.holidays('2012', '2016')\n",
        "daily = daily.join(pd.Series(1, index=holidays, name='holiday'))\n",
        "daily['holiday'].fillna(0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "LcGeqraOR23W"
      },
      "source": [
        "We also might suspect that the hours of daylight would affect how many people ride; let's use the standard astronomical calculation to add this information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "anYH4iJMR23a"
      },
      "source": [
        "def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
        "    \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
        "    days = (date - pd.datetime(2000, 12, 21)).days\n",
        "    m = (1. - np.tan(np.radians(latitude))\n",
        "         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
        "    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
        "\n",
        "daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index))\n",
        "daily[['daylight_hrs']].plot()\n",
        "plt.ylim(8, 17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Qix5hMrYR23j"
      },
      "source": [
        "We can also add the average temperature and total precipitation to the data.\n",
        "In addition to the inches of precipitation, let's add a flag that indicates whether a day is dry (has zero precipitation):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "akwgV30KR23m"
      },
      "source": [
        "# temperatures are in 1/10 deg C; convert to C\n",
        "weather['TMIN'] /= 10\n",
        "weather['TMAX'] /= 10\n",
        "weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])\n",
        "\n",
        "# precip is in 1/10 mm; convert to inches\n",
        "weather['PRCP'] /= 254\n",
        "weather['dry day'] = (weather['PRCP'] == 0).astype(int)\n",
        "\n",
        "daily = daily.join(weather[['PRCP', 'Temp (C)', 'dry day']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "zfSflTlGR23w"
      },
      "source": [
        "Finally, let's add a counter that increases from day 1, and measures how many years have passed.\n",
        "This will let us measure any observed annual increase or decrease in daily crossings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Vo-2TnzUR23y"
      },
      "source": [
        "daily['annual'] = (daily.index - daily.index[0]).days / 365."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "LHxPxGQ2R239"
      },
      "source": [
        "Now our data is in order, and we can take a look at it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6h4hNQr-R23_"
      },
      "source": [
        "print(\"Dataset shape:\", daily.shape)\n",
        "daily.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jRHx_10oR24J"
      },
      "source": [
        "Since we would like to test how well our predictive model would perform in new data, we need to test it performance with data that it has not seen before. To achieve this, we will first partition our dataset in a training and a test set. We will do an 70/30 partition using [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from `sklearn.model_selection` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcfVut9fsEWb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "daily, test = train_test_split(daily, test_size=0.3, random_state=43)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si-dT8RI0Z4b"
      },
      "source": [
        "**Let's do some EDA.**\n",
        "\n",
        "You should never look at the values of your test set to avoid any bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yr3xUDq0mRe"
      },
      "source": [
        "daily.describe(include='all')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF3tptzx03Fi"
      },
      "source": [
        "daily.describe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-75JPwh09zi"
      },
      "source": [
        "As you can see from the EDA, we should probably perform feature normalization, as well as to deal with the NA's value. There are [multiple techniques to deal with NA's](https://medium.com/@george.drakos62/handling-missing-values-in-machine-learning-part-1-dda69d4f88ca), but for now we can just remove any row that has an NA value. We should also do the same for our test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2PINXx7AR24M"
      },
      "source": [
        "# Drop any rows with null values\n",
        "daily.dropna(axis=0, how='any', inplace=True)\n",
        "test.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "column_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday',\n",
        "                'daylight_hrs', 'PRCP', 'dry day', 'Temp (C)', 'annual']\n",
        "X = daily[column_names]\n",
        "y = daily['Total']\n",
        "\n",
        "X_test = test[column_names]\n",
        "y_test = test['Total']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "qJdP4DoDR24U"
      },
      "source": [
        "Now can perform some Linear Regression and see how well were are able to predicted bicycle traffic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvVXpz8H6lAH"
      },
      "source": [
        "### 2.1- Multivariate Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "M4HyyZybR24X"
      },
      "source": [
        "#Fit Multivariate Linear Regression Model\n",
        "model = LinearRegression(fit_intercept=False)\n",
        "model.fit(X, y)\n",
        "\n",
        "#Calculate the model's parameters error\n",
        "np.random.seed(1)\n",
        "err = np.std([model.fit(*resample(X, y)).coef_\n",
        "              for i in range(1000)], 0)\n",
        "\n",
        "params = pd.Series(model.coef_, index=X.columns)\n",
        "print(pd.DataFrame({'effect': params.round(0),\n",
        "                    'error': err.round(0)}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaM-agDB59ON"
      },
      "source": [
        "\n",
        "Here we are looking a the  model's parameters (`effect`) with a measuremnt of their uncertainty (`error`). We computed these uncertainties by using bootstrap resamplings of the data. However, there are some other statistical ways of doing this. (see Trevor Hastie, Robert Tibshirani and Jerome Friedman (2009). Elements of Statistical Learning. Chapter 3)\n",
        "\n",
        "By lookin at the model's parameter, we first see that there is a relatively stable trend in the weekly baseline: there are many more riders on weekdays than on weekends and holidays.\n",
        "\n",
        "We see that for each additional hour of daylight, 240 ± 22 more people choose to ride; a temperature increase of one degree Celsius encourages 133 ± 9 people to grab their bicycle; and each inch of precipitation means 1362 ± 158 more people leave their bike at home.\n",
        "\n",
        "Once all these effects are accounted for, we see a modest increase of 78 ± 41 new daily riders each year.\n",
        "\n",
        "We can now calculate some performance metrics and visually inspect how well our Linear Regression model did on the training set and testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up_tS2fG5-Ag"
      },
      "source": [
        "RMSE_training_LR=np.sqrt(mean_squared_error(y, model.predict(X)))\n",
        "print(\"RMSE from Training=\" + str(round(RMSE_training_LR, 4)))\n",
        "\n",
        "daily['predicted'] = model.predict(X)\n",
        "daily[['Total', 'predicted']].plot(alpha=0.5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9lNN54-6HzA"
      },
      "source": [
        "test['predicted'] = model.predict(X_test)\n",
        "test[['Total', 'predicted']].plot(alpha=0.5);\n",
        "\n",
        "RMSE_test_LR=np.sqrt(mean_squared_error(y_test, model.predict(X_test)))\n",
        "print(\"RMSE from Test=\" + str(round(RMSE_test_LR,4)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4cCKSdB6PmG"
      },
      "source": [
        "It is evident that we have missed some key features, especially during the summer time.\n",
        "Either our features are not complete (i.e., people decide whether to ride to work based on more than just these) or there are some nonlinear relationships that we have failed to take into account (e.g., perhaps people ride less at both high and low temperatures).\n",
        "Nevertheless, our rough approximation is enough to give us some insights, and we can take a look at the coefficients of the linear model to estimate how much each feature contributes to the daily bicycle count:\n",
        "\n",
        "Our model is almost certainly missing some relevant information. For example, nonlinear effects (such as effects of precipitation *and* cold temperature) and nonlinear trends within each variable (such as disinclination to ride at very cold and very hot temperatures) cannot be accounted for in this model.\n",
        "\n",
        "Additionally, we have thrown away some of the finer-grained information (such as the difference between a rainy morning and a rainy afternoon), and we have ignored correlations between days (such as the possible effect of a rainy Tuesday on Wednesday's numbers, or the effect of an unexpected sunny day after a streak of rainy days).\n",
        "\n",
        "These are all potentially interesting effects, and you now have the tools to begin exploring them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i7M6gQcoMDj"
      },
      "source": [
        "### 2.2- Ridge Linear Regression Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V35cgHyQ_RQZ"
      },
      "source": [
        "Now is your time to implement a Ridge Linear Regression using skelearn's [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). You could also see the last code chunck of the [Regularized_Linear_Regression.ipynb notebook](https://colab.research.google.com/github/lopezbec/intro_python_notebooks/blob/master/Regularized_Linear_Regression.ipynb)\n",
        "\n",
        "You can perform some additional feature engineering. BUT ONLY LOOK AT THE PERFORMANCE OF YOUR MODEL ON THE TEST SET ONCE YOUR MODEL IS READY (i.e., do not change your model after looking at the test set performance since this will introduce bias)\n",
        "\n",
        "The name of the pipepline should be `ridge_regression_pipeline`, also be advise to keep the \"degree of polynomial\"   small (~<4) since this dataset already have a lot of features and incresing it even more, will increase the compute time + the bootstrap resamplings  will take even longer. (for example degree 2 takes ~20s but with 4 it jumps to ~254s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJSXWsXHnpwY"
      },
      "source": [
        "\n",
        "### START CODE HERE ### (≈ 9 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "#Calculate the model's parameters error\n",
        "np.random.seed(1)\n",
        "err = np.std([ridge_regression_pipeline.fit(*resample(X, y)).named_steps.Ridge_lin_reg.coef_\n",
        "              for i in range(1000)], 0)\n",
        "\n",
        "params = pd.Series(ridge_regression_pipeline.named_steps.Ridge_lin_reg.coef_, \n",
        "                   index=ridge_regression_pipeline.named_steps.poly_features.get_feature_names(X.columns))\n",
        "pd.DataFrame({'effect': params.round(0),'error': err.round(0)})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkZpyQypCPXR"
      },
      "source": [
        "#Calculate model RMSE\n",
        "RMSE_training_Ridge=np.sqrt(mean_squared_error(y, ridge_regression_pipeline.predict(X)))\n",
        "print(\"RMSE Ridge LR from Training=\" + str(round(RMSE_training_Ridge,4)))\n",
        "\n",
        "#Plot predicted values \n",
        "daily['predicted'] = ridge_regression_pipeline.predict(X)\n",
        "daily[['Total', 'predicted']].plot(alpha=0.5);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5wpumSe-GrS"
      },
      "source": [
        "RMSE_test_Ridge=np.sqrt(mean_squared_error(y_test, ridge_regression_pipeline.predict(X_test)))\n",
        "print(\"RMSE Ridge LR from Test=\" + str(round(RMSE_test_Ridge,4)))\n",
        "\n",
        "test['predicted'] = ridge_regression_pipeline.predict(X_test)\n",
        "test[['Total', 'predicted']].plot(alpha=0.5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy54ucAZ-llZ"
      },
      "source": [
        "### 2.3- Lasso Linear Regression Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM5LUl5zE5m4"
      },
      "source": [
        "Now is your time to implement a Lasso Linear Regression using skelearn's [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). You could also see the last code chunck of the [Regularized_Linear_Regression.ipynb notebook](https://colab.research.google.com/github/lopezbec/intro_python_notebooks/blob/master/Regularized_Linear_Regression.ipynb)\n",
        "\n",
        "You can perform some additional feature engineering. BUT ONLY LOOK AT THE PERFORMANCE OF YOUR MODEL ON THE TEST SET ONCE YOU MODEL IS READY (i.e., do not change your model after looking at the test set performance since this will introduce bias).\n",
        "\n",
        "The name of the pipepline should be `lasso_regression_pipeline` \n",
        "(due to the issues of converganve, Lasso might take longer than Ridge LR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzKZvZMI42Cy"
      },
      "source": [
        "### START CODE HERE ### (≈ 9 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "np.random.seed(1)\n",
        "err = np.std([lasso_regression_pipeline.fit(*resample(X, y)).named_steps.Lasso_lin_reg.coef_\n",
        "              for i in range(1000)], 0)\n",
        "\n",
        "params = pd.Series(lasso_regression_pipeline.named_steps.Lasso_lin_reg.coef_, \n",
        "                   index=lasso_regression_pipeline.named_steps.poly_features.get_feature_names(X.columns))\n",
        "table=pd.DataFrame({'effect': params.round(0),'error': err.round(0)})\n",
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgrRcTMU-C3x"
      },
      "source": [
        "Lets get the features with non-zero weights/coefficients "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UBpm2Md8hJN"
      },
      "source": [
        "table[ table['effect']!=0 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mCpWYxpERXf"
      },
      "source": [
        "RMSE_training_Lasso=np.sqrt(mean_squared_error(y, lasso_regression_pipeline.predict(X)))\n",
        "print(\"RMSE from Training=\" + str(round(RMSE_training_Lasso,4)))\n",
        "\n",
        "daily['predicted'] = lasso_regression_pipeline.predict(X)\n",
        "daily[['Total', 'predicted']].plot(alpha=0.5);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE4Y_RQjDCqH"
      },
      "source": [
        "\n",
        "RMSE_test_Lasso=np.sqrt(mean_squared_error(y_test, lasso_regression_pipeline.predict(X_test)))\n",
        "print(\"RMSE from Test=\" + str(round(RMSE_test_Lasso,4)))\n",
        "\n",
        "test['predicted'] = lasso_regression_pipeline.predict(X_test)\n",
        "test[['Total', 'predicted']].plot(alpha=0.5);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubozIj4KGUXO"
      },
      "source": [
        "As you have experienced, we can do a lot of different things to make our model perform better (change the regularization term, add or remove features,….). But how can we make this process more efficient? \n",
        "\n",
        "\n",
        "Well, we could use a process to identify the hyperparameter that will create the model that best fit the data, similar to the training process that helps us identify the model’ parameter that best fit the data (we will talk more about hyperparameter optimization/tuning later on the class) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr5d-t8sX4bD"
      },
      "source": [
        "###### **DO NOT DELETE NOR MODIFY THESE CODE CELLS**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha9RZa_2FuWc"
      },
      "source": [
        "# # ###DO NOT DELETE NOR MODIFY THIS CODE CELL####\n",
        "# ###DO NOT DELETE NOR MODIFY THIS CODE CELL####\n",
        "!wget https://raw.githubusercontent.com/lopezbec/intro_python_notebooks/main/GRADING_MRLR.py\n",
        "import numpy as np\n",
        "from GRADING_MRLR import GRADING\n",
        "\n",
        "try:\n",
        "    X_norm_sklearn\n",
        "except:\n",
        "    X_norm_sklearn=None\n",
        "try:\n",
        "    X_norm\n",
        "except:\n",
        "    X_norm=None\n",
        "try:\n",
        "    theta_best\n",
        "except:\n",
        "    theta_best=None\n",
        "try:\n",
        "    theta_cost\n",
        "except:\n",
        "    theta_cost=None\n",
        "try:\n",
        "    MAE_val\n",
        "except:\n",
        "    MAE_val=None\n",
        "try:\n",
        "    MAE_skl\n",
        "except:\n",
        "    MAE_skl=None\n",
        "try:\n",
        "    MSE_skl\n",
        "except:\n",
        "    MSE_skl=None\n",
        "try:\n",
        "    RMSE_val\n",
        "except:\n",
        "    RMSE_val=None\n",
        "try:\n",
        "    RMSE_skl\n",
        "except:\n",
        "    RMSE_skl=None\n",
        "try:\n",
        "    R_square_val\n",
        "except:\n",
        "    R_square_val=None\n",
        "\n",
        "try:\n",
        "    R_square_skl\n",
        "except:\n",
        "    R_square_skl=None\n",
        "\n",
        "try:\n",
        "    ridge_regression_pipeline\n",
        "except:\n",
        "    ridge_regression_pipeline=None\n",
        "\n",
        "\n",
        "try:\n",
        "    RMSE_training_Ridge\n",
        "except:\n",
        "    RMSE_training_Ridge=None\n",
        "\n",
        "\n",
        "try:\n",
        "    RMSE_test_Ridge\n",
        "except:\n",
        "    RMSE_test_Ridge=None\n",
        "\n",
        "try:\n",
        "    RMSE_training_Lasso\n",
        "except:\n",
        "    RMSE_training_Lasso=None\n",
        "\n",
        "\n",
        "try:\n",
        "    RMSE_test_Lasso\n",
        "except:\n",
        "    RMSE_tRMSE_test_Lassoest_Ridge=None\n",
        "\n",
        "\n",
        "try:\n",
        "    lasso_regression_pipeline\n",
        "except:\n",
        "    lasso_regression_pipeline=None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "GRADING(X_norm_sklearn, X_norm,theta_best, theta_cost,MAE_val, MAE_skl,MSE_val, MSE_skl,RMSE_val,RMSE_skl,R_square_val,R_square_skl,RMSE_training_LR,RMSE_test_LR,\n",
        "        ridge_regression_pipeline,RMSE_training_Ridge, RMSE_test_Ridge,RMSE_training_Lasso, RMSE_test_Lasso,lasso_regression_pipeline)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}