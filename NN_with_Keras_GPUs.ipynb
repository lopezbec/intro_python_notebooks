{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nav_menu": {
      "height": "264px",
      "width": "369px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF6Lm26UO4TQ"
      },
      "source": [
        "\n",
        "<center><b>© Content is made available under the CC-BY-NC-ND 4.0 license. Christian Lopez, lopezbec@lafayette.edu<b><center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/lopezbec/intro_python_notebooks/blob/master/NN_with_Keras_GPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td>\n",
        "         </table>\n",
        "          <br><br></br>"
      ],
      "metadata": {
        "id": "viMWVVMLX6MX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HuUEdPgNCSo"
      },
      "source": [
        "#Training NN with GPUs\n",
        "\n",
        "This Notebook will give you an introduction on how to train a Neural Network using GPU in Google Colab.  \n",
        "\n",
        "In this Notebook you will learn:\n",
        "\n",
        "- The benefits of using GPUs to train NN\n",
        "- Think to consider when training with GPUs\n",
        "- The value of Deep NN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###**Before we start:**\n",
        "Make sure you are using GPUs as your Hardware accelerator, go to `Runtime`>`Change runtime type`> `Run time type`.\n",
        "\n",
        "\n",
        "In Google Colab is extremely easy to enable a GPU and train with it. However, if you have a CUDA enable GPU in your computer (or in a server) and you would like to use it instead, the process is a bit more complex (see a [tutorial here](https://medium.com/@ab9.bhatia/set-up-gpu-accelerated-tensorflow-keras-on-windows-10-with-anaconda-e71bfa9506d1)).\n",
        "\n",
        "\n",
        "Most of the notebooks we are going to be using are inspired from existing notebooks that are available online and are made free for educational purposes. Nonetheless, these notebooks should not be share without prior permission of the instructor. When working in an assignment always remember the [Student Code of Conduct](https://conduct.lafayette.edu/student-handbook/student-code-of-conduct/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoL940bWO4Ta"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ba4UQLxP7fCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVSnGN77O4Tg"
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "from tensorflow import keras\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import time\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
        "import logging\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "\n",
        "!pip install Pillow\n",
        "from PIL import Image\n",
        "import imageio\n",
        "\n",
        "# GPU memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiQz-MMCVovd"
      },
      "source": [
        "## 1- Google Colab GPU information\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU_LH-Gv25La"
      },
      "source": [
        "You might encounter more issues and difficulties when using GPU in Colab than CPU. You might need to `Factory reset runtime` and/or `Restart runtime` more frequently than before, or manually go over the code cell and run them `Shift+ENTER` (a small price to paid for the convenience of a free GPU).\n",
        "\n",
        "Let's check how many GPUs do we have:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2BQSIW23pVn"
      },
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB4TXw2I9VjU"
      },
      "source": [
        "What type of CPU and GPU do we have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwjWAJ7G8veB"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XminA2ML-F5y"
      },
      "source": [
        "The default GPU for Google Colab is an [NVIDIA Tesla K80]( https://www.nvidia.com/en-gb/data-center/tesla-k80/)....but the [legend says]( https://www.reddit.com/r/MachineLearning/comments/duds5d/d_colab_has_p100_gpus/) that once in a blue moons the rare [NVIDIA P100]( https://www.nvidia.com/en-us/data-center/tesla-p100/) might show up and help you train your NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fiMM9q9H__Z"
      },
      "source": [
        "Let’s look at how much memory we have in our VM and GPU. This will ultimately constraint how many training samples we can load at once to the GPU to train our models.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0H2HIqvB18S"
      },
      "source": [
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"VM RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.2f}GB | Used: {3:.2f} GB | Util {2:3.2f}% | Total {3:.2f} GB\".format(gpu.memoryFree/1000, gpu.memoryUsed/1000, gpu.memoryUtil*100, gpu.memoryTotal/1000))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00I89R00YhSq"
      },
      "source": [
        "## 2-  Training on the MNIST Fashion dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sgaflt2O4VH"
      },
      "source": [
        "We will use the same dataset as before to help us compare the speed of CPU vs GPU. Let's start by loading the fashion MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqAUiXDqO4VJ"
      },
      "source": [
        "#load dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#Split and Scale dataset\n",
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255.\n",
        "\n",
        "#Here are the corresponding class names:\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3IcA65gO4WC"
      },
      "source": [
        "Let's take a look at a sample of the images in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn3ugDIcO4WD"
      },
      "source": [
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyr7XsKe-SJO"
      },
      "source": [
        "### 2.1- Building, Training, and Testing L=3 NN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhvVDzDuI5sO"
      },
      "source": [
        "This is the same NN architecture, and training process we used in the last notebook. It will still take some time, but not as much as with a CPU!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5gj1aIVccyY"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# specify our Neural Network architecture\n",
        "model_f = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "#Compile the  model\n",
        "model_f.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "#Lets save our initial model weights so we can \"reset\" the traning latter\n",
        "Wsave = model_f.get_weights().copy()\n",
        "\n",
        "print(model_f.summary())\n",
        "#lets plot the model info\n",
        "keras.utils.plot_model(model_f, show_shapes=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD8BBQ1SBFmH"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model_f.set_weights(Wsave)\n",
        "\n",
        "#Train the model\n",
        "tic = time.process_time()\n",
        "history_nn_f = model_f.fit(X_train, y_train, epochs=30, batch_size=50, validation_data=(X_valid, y_valid), verbose=1)\n",
        "toc = time.process_time()\n",
        "#Model evaluation\n",
        "test_nn_f=model_f.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "#Plot performnace\n",
        "pd.DataFrame(history_nn_f.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "time_nn_f=toc-tic\n",
        "print(\"Time to train the model: {0:.4f} secs \".format((time_nn_f)))\n",
        "print(\"Training Set Accuracy:   {0:.4%} & Loss:{1:.4f}\".format(history_nn_f.history['accuracy'][29],history_nn_f.history['loss'][29]))\n",
        "print(\"Validation Set Accuracy: {0:.4%} & Loss:{1:.4f}\".format(history_nn_f.history['val_accuracy'][29],history_nn_f.history['val_loss'][29]))\n",
        "print(\"Test Set Accuracy:       {0:.4%} & Loss:{1:.4f}\".format(test_nn_f[1],test_nn_f[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "205ixaiJb8I4"
      },
      "source": [
        "If you compare these results with the results from our previous notebook, where we used CPU to train this same NN architecture, you will notice:\n",
        "- The time to train the model was cut by more than half\n",
        "- The Accuracy and Lost are close to the CPU results, but not the same.\n",
        "\n",
        "If you restart the environment and run all the code cell above (`Runtime`>` Restart runtime…`>`Run before`)You will notice that you get some results a bit difference time over time, even if we are setting `np.random.seed(42)` and `tf.random.set_seed(42)`. This is because the GPU might be using some *“sophisticated stack of GPU libraries, and some of these may introduce their own source of randomness that you may or may not be able to account“* ( [see Randomness from Using the GPU)]( https://machinelearningmastery.com/reproducible-results-neural-networks-keras/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJKyDrMmlcqX"
      },
      "source": [
        "### 2.2- Building, Training, and Testing Softmax Regression as a NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqVy_5XVlwXj"
      },
      "source": [
        "Let see how would a simple Softmax Regression model will perform in this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVxDpIN6lw1T"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# specify our Neural Network architecture\n",
        "Log_model_f = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "#Compile the  model\n",
        "Log_model_f.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Lets save our initial model weights so we can \"reset\" the traning latter\n",
        "Wsave = Log_model_f.get_weights().copy()\n",
        "\n",
        "print(Log_model_f.summary())\n",
        "#lets plot the model info\n",
        "keras.utils.plot_model(Log_model_f, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVR7b_eol3St"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "Log_model_f.set_weights(Wsave)\n",
        "\n",
        "#Train the model\n",
        "tic = time.process_time()\n",
        "history_log_f = Log_model_f.fit(X_train, y_train, epochs=30, batch_size=50, validation_data=(X_valid, y_valid), verbose=0)\n",
        "toc = time.process_time()\n",
        "#Model evaluation\n",
        "test_log_f=Log_model_f.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "#Plot performnace\n",
        "import pandas as pd\n",
        "pd.DataFrame(history_log_f.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "time_log_f=toc-tic\n",
        "print(\"Time to train the model: {0:.4f} secs \".format((time_log_f)))\n",
        "print(\"Training Set Accuracy:   {0:.4%} & Loss:{1:.4f}\".format(history_log_f.history['accuracy'][29],history_log_f.history['loss'][29]))\n",
        "print(\"Validation Set Accuracy: {0:.4%} & Loss:{1:.4f}\".format(history_log_f.history['val_accuracy'][29],history_log_f.history['val_loss'][29]))\n",
        "print(\"Test Set Accuracy:       {0:.4%} & Loss:{1:.4f}\".format(test_log_f[1],test_log_f[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNwP6eCFpMYK"
      },
      "source": [
        "\n",
        "If you compare these results with the previous L=3 NN results, a model that had 250k more parameters, you will notice:\n",
        "\n",
        "- The time to train is less, but not by much\n",
        "- The model performed a bit worse, but not much worse\n",
        "\n",
        "This is because the NN take advantage of the GPU ability to perform matrix and vector multiplication. Lastly, the Softmax Regression is an enough complex model to capture most of the variability in the dataset (i.e., patterns of the dataset). You could even use a TPU to train a more complex model faster, but you need to update the code bit (see [tutorial here](https://heartbeat.fritz.ai/step-by-step-use-of-google-colab-free-tpu-75f8629492b3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGDc2FyEmBKk"
      },
      "source": [
        "## 3-  Training on the MNIST Digits dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8HqiABsdt2g"
      },
      "source": [
        "\n",
        "#load dataset\n",
        "fashion_mnist = keras.datasets.mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#Split and Scale dataset\n",
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255.\n",
        "\n",
        "#Here are the corresponding class names:\n",
        "class_names = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajvq8rvL08AP"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqhJATAjdtVO"
      },
      "source": [
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0aajOIcr1rO"
      },
      "source": [
        "### 3.1- Building, Training, and Testing L=3 NN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWqoP-Q6sCCA"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# specify our Neural Network architecture\n",
        "model_nn = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "#Compile the  model\n",
        "model_nn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "#Lets save our initial model weights so we can \"reset\" the traning latter\n",
        "Wsave = model_nn.get_weights().copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEcH1IBJsB2w"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model_nn.set_weights(Wsave)\n",
        "\n",
        "#Train the model\n",
        "tic = time.process_time()\n",
        "history_nn = model_nn.fit(X_train, y_train, epochs=30, batch_size=50, validation_data=(X_valid, y_valid), verbose=0)\n",
        "toc = time.process_time()\n",
        "#Model evaluation\n",
        "test_nn=model_nn.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "#Plot performnace\n",
        "pd.DataFrame(history_nn.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "time_nn=toc-tic\n",
        "print(\"Time to train the model: {0:.4f} secs \".format((time_nn)))\n",
        "print(\"Training Set Accuracy:   {0:.4%} & Loss:{1:.4f}\".format(history_nn.history['accuracy'][29],history_nn.history['loss'][29]))\n",
        "print(\"Validation Set Accuracy: {0:.4%} & Loss:{1:.4f}\".format(history_nn.history['val_accuracy'][29],history_nn.history['val_loss'][29]))\n",
        "print(\"Test Set Accuracy:       {0:.4%} & Loss:{1:.4f}\".format(test_nn[1],test_nn[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJqUS9psr8oX"
      },
      "source": [
        "### 3.2- Building, Training, and Testing Softmax Regression as a NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miLDyCrSsSnh"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# specify our Neural Network architecture\n",
        "Log_model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "#Compile the  model\n",
        "Log_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Lets save our initial model weights so we can \"reset\" the traning latter\n",
        "Wsave = Log_model.get_weights().copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C3J3JoAsb6w"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "Log_model.set_weights(Wsave)\n",
        "\n",
        "#Train the model\n",
        "tic = time.process_time()\n",
        "history_log = Log_model.fit(X_train, y_train, epochs=30, batch_size=50, validation_data=(X_valid, y_valid), verbose=0)\n",
        "toc = time.process_time()\n",
        "#Model evaluation\n",
        "test_log=Log_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "#Plot performnace\n",
        "import pandas as pd\n",
        "pd.DataFrame(history_log.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "time_log=toc - tic\n",
        "print(\"Time to train the model: {0:.4f} secs \".format((time_log)))\n",
        "print(\"Training Set Accuracy:   {0:.4%} & Loss:{1:.4f}\".format(history_log.history['accuracy'][29],history_log.history['loss'][29]))\n",
        "print(\"Validation Set Accuracy: {0:.4%} & Loss:{1:.4f}\".format(history_log.history['val_accuracy'][29],history_log.history['val_loss'][29]))\n",
        "print(\"Test Set Accuracy:       {0:.4%} & Loss:{1:.4f}\".format(test_log[1],test_log[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEXM6vBctWG8"
      },
      "source": [
        "# Now I want you to look at all the results and think:\n",
        "-\tWhy the time to train the models did not change as much between the dataset\n",
        "-\tWhy the L=3 NN and Softmax Regression performed better in the Digits dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHofWYtGucrm"
      },
      "source": [
        "print(\"                   MNIST FASHION DATASET\")\n",
        "print(\"   L3 NN: n[1]=300 n[2]=100\")\n",
        "print(\"Time to train the model: {0:.4f} secs \".format((time_nn_f)))\n",
        "print(\"Training Set Accuracy:   {0:.4%} & Loss:{1:.4f}\".format(history_nn_f.history['accuracy'][29],history_nn_f.history['loss'][29]))\n",
        "print(\"Validation Set Accuracy: {0:.4%} & Loss:{1:.4f}\".format(history_nn_f.history['val_accuracy'][29],history_nn_f.history['val_loss'][29]))\n",
        "print(\"Test Set Accuracy:       {0:.4%} & Loss:{1:.4f}\".format(test_nn_f[1],test_nn_f[0]))\n",
        "print(\"----------------------------------------------------------------------------------\")\n",
        "print(\"    SoftMax\")\n",
        "print(\"Time to train the model: {0:.4f} secs \".format((time_log_f)))\n",
        "print(\"Training Set Accuracy:   {0:.4%} & Loss:{1:.4f}\".format(history_log_f.history['accuracy'][29],history_log_f.history['loss'][29]))\n",
        "print(\"Validation Set Accuracy: {0:.4%} & Loss:{1:.4f}\".format(history_log_f.history['val_accuracy'][29],history_log_f.history['val_loss'][29]))\n",
        "print(\"Test Set Accuracy:       {0:.4%} & Loss:{1:.4f}\".format(test_log_f[1],test_log_f[0]))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"                  MNIST DIGITS DATASET\")\n",
        "print(\"   L3 NN: n[1]=300 n[2]=100\")\n",
        "print(\"Time to train the model: {0:.4f} secs \".format((time_nn)))\n",
        "print(\"Training Set Accuracy:   {0:.4%} & Loss:{1:.4f}\".format(history_nn.history['accuracy'][29],history_nn.history['loss'][29]))\n",
        "print(\"Validation Set Accuracy: {0:.4%} & Loss:{1:.4f}\".format(history_nn.history['val_accuracy'][29],history_nn.history['val_loss'][29]))\n",
        "print(\"Test Set Accuracy:       {0:.4%} & Loss:{1:.4f}\".format(test_nn[1],test_nn[0]))\n",
        "print(\"----------------------------------------------------------------------------------\")\n",
        "print(\"    SoftMax\")\n",
        "print(\"Time to train the model: {0:.4f} secs \".format((time_log)))\n",
        "print(\"Training Set Accuracy:   {0:.4%} & Loss:{1:.4f}\".format(history_log.history['accuracy'][29],history_log.history['loss'][29]))\n",
        "print(\"Validation Set Accuracy: {0:.4%} & Loss:{1:.4f}\".format(history_log.history['val_accuracy'][29],history_log.history['val_loss'][29]))\n",
        "print(\"Test Set Accuracy:       {0:.4%} & Loss:{1:.4f}\".format(test_log[1],test_log[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-  Other Keras and TensorFlow features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_6adTA6c3IMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a lot of interesting features from Keras and TensorFLow that make it very easy to train large NN and keep track of everything. For example:"
      ],
      "metadata": {
        "id": "n5uWO0AP3X2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  4.1 -Builing larger NN using loops"
      ],
      "metadata": {
        "id": "6S0K3BMU3lYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow and check version for confirmation\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Import necessary modules from tensorflow.keras\n",
        "from tensorflow.keras import regularizers, backend, models, layers\n",
        "\n",
        "# Instead of importing from tensorflow.keras.wrappers.scikit_learn, use scikeras:\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Clear previous sessions and set seeds for reproducibility\n",
        "backend.clear_session()\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define the model building function\n",
        "def build_model(n_hidden=20, n_neurons=20, reg=0, input_shape=[28, 28]):\n",
        "    model = models.Sequential()\n",
        "    # Flatten the input (e.g., 28x28 image) to a 1D vector\n",
        "    model.add(layers.Flatten(input_shape=input_shape))\n",
        "    # Add hidden Dense layers with ReLU activation and L2 regularization\n",
        "    for _ in range(n_hidden):\n",
        "        model.add(layers.Dense(n_neurons, activation=\"relu\",\n",
        "                               kernel_regularizer=regularizers.l2(reg)))\n",
        "    # Output layer for 10 classes with softmax activation\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    # Compile the model with SGD optimizer and sparse categorical crossentropy loss\n",
        "    optimizer = \"sgd\"\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Wrap the model with scikeras' KerasClassifier for scikit-learn compatibility\n",
        "keras_reg = KerasClassifier(model=build_model)\n"
      ],
      "metadata": {
        "id": "rZ2zXIb4ywIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  4.2 -Adding \"callbacks\""
      ],
      "metadata": {
        "id": "XTJn-rQJ3yCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding a [Keras callbacks](https://keras.io/api/callbacks/) to your models would allow you to do a lot of different things, like keep a log of your training process just in case your systems crashes, save the best model so you can share it with others, and use TensorBoard to visualize your model process as it trains\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aA-GlliH3446"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Using CIFAR-10 dataset as an example. It has images of shape (32, 32, 3).\n",
        "(X_train, y_train), (X_valid, y_valid) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "X_train = X_train.astype(\"float32\") / 255.\n",
        "X_valid = X_valid.astype(\"float32\") / 255.\n",
        "\n",
        "\n",
        "# Create a Sequential model with an input layer that matches the CIFAR-10 image shape.\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Input(shape=(32, 32, 3)),  # Matching the input shape of CIFAR-10 images\n",
        "    keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
        "    keras.layers.MaxPooling2D(2, 2),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    keras.layers.MaxPooling2D(2, 2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(64, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")  # 10 classes for CIFAR-10\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "# Create a directory for saving log files (if it doesn't exist)\n",
        "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
        "if not os.path.exists(root_logdir):\n",
        "    os.makedirs(root_logdir)\n",
        "\n",
        "def get_run_logdir():\n",
        "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "    return os.path.join(root_logdir, run_id)\n",
        "\n",
        "run_logdir = get_run_logdir()\n",
        "\n",
        "# Callback to save the best model during training\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
        "\n",
        "# TensorBoard callback to log training events (adjust log directory)\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(log_dir=run_logdir)\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=50,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint_cb, tensorboard_cb]\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bDoVGagK-zsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  4.3 -[TensorBoard](https://keras.io/api/callbacks/tensorboard/)"
      ],
      "metadata": {
        "id": "XTTwiKds4s_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./my_logs --port=6006"
      ],
      "metadata": {
        "id": "IMD25jtb-oPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5brFZqFIkf-"
      },
      "source": [
        "## 5-  Training on a color image dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXkL9Zv-y1_B"
      },
      "source": [
        "\n",
        "#load dataset\n",
        "cifar10 = keras.datasets.cifar10\n",
        "(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
        "y_train_full=y_train_full.ravel()\n",
        "y_test=y_test.ravel()\n",
        "#Split and Scale dataset\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test\n",
        "\n",
        "#Here are the corresponding class names:\n",
        "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "clear_output()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6H2OOhvzGDW"
      },
      "source": [
        "n_rows = 4\n",
        "n_cols = 4\n",
        "plt.figure(figsize=(n_cols * 3, n_rows * 3))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RybHrkss4STi"
      },
      "source": [
        "#### **Exercise**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_QX4QYY4P9g"
      },
      "source": [
        "Now use the [CIFAR10](https://keras.io/datasets/) to train a NN of your choice. Make sure to:\n",
        "\n",
        "-\tMake sure your model does not take more than 5mins to train (using GPU) and print the training time (use `tic = time.process_time()` `toc = time.process_time()`    )\n",
        "- Make sure to print the Accuracy and Loss for each of the datasets\n",
        "-\tMake sure to plot the Loss and performance metric of your model\n",
        "-Explain in a text cell how is your model performing (over/underfitting)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8avqMw3zFls"
      },
      "source": [
        "############################# START CODE HERE #############################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################# END CODE HERE #############################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain in a text cell how is your model performing (over/underfitting) and what you should do:\n"
      ],
      "metadata": {
        "id": "94_a4w3yZ83c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd8Oz92l-eRO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVmiCdzpMY9t"
      },
      "source": [
        "### You can now test the model in your own image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "61k1cPlySsa0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ45OUQs-6k5"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbTs_lln3Di2"
      },
      "source": [
        "\n",
        "#(PUT YOUR IMAGE NAME)\n",
        "fname = \"_________.jpg\"   # change this to the name of your image file\n",
        "\n",
        "# We preprocess the image to fit your algorithm (*64*64 pixels).\n",
        "\n",
        "img = Image.open(fname)\n",
        "img.show()\n",
        "new_image = img.resize((32, 32))\n",
        "data = np.asarray(new_image, dtype=\"int32\" )\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(data)\n",
        "\n",
        "\n",
        "img = Image.open(fname)\n",
        "new_image = img.resize((32, 32))\n",
        "data = np.asarray(new_image, dtype=\"float64\" )\n",
        "data=np.array([data/255.])\n",
        "prediction=model.predict(data)\n",
        "print(\"predicted class:  \" +class_names[np.argmax(prediction)] )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}