{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nav_menu": {
      "height": "264px",
      "width": "369px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Jy5f7p4SGizB"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF6Lm26UO4TQ"
      },
      "source": [
        "\n",
        "<center><b>© Content is made available under the CC-BY-NC-ND 4.0 license. Christian Lopez, lopezbec@lafayette.edu<b><center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/lopezbec/intro_python_notebooks/blob/master/Perceptrons_and_NN_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td>\n",
        "         </table>\n",
        "          <br><br></br> \n"
      ],
      "metadata": {
        "id": "aXlB_IAuVN5N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HuUEdPgNCSo"
      },
      "source": [
        "#Implementing NN with Keras and TensorFlow \n",
        "\n",
        "This Notebook will give you an introduction on how to train a  Neural Network using Keras and TensorFlow.  \n",
        "\n",
        "In this Notebook you will learn:\n",
        "\n",
        "- How to implement a Perceptron\n",
        "- How to implement multiple activation function and algorithmically get their derivatives\n",
        "- How to build, train, and test a NN for image classification \n",
        "\n",
        "Most of the notebooks we are going to be using are inspired from existing notebooks that are available online and are made free for educational purposes. Nonetheless, these notebooks should not be share without prior permission of the instructor. When working in an assignment always remember the [Student Code of Conduct](https://conduct.lafayette.edu/student-handbook/student-code-of-conduct/).\n",
        "\n",
        "*The following sections were inspired and uses some of the code and text from the book:\n",
        "\n",
        "Géron, A. (2019) 2nd Ed. Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. O'Reilly Media, Inc.( ISBN-10: 1491962291) [Chapter 10](https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoL940bWO4Ta"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVSnGN77O4Tg"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import time\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
        "import logging\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT2scaYRO4Tr"
      },
      "source": [
        "#1- Perceptrons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh-KD05gQVFn"
      },
      "source": [
        "The Perceptron is one of the simplest ANN architectures. It is based on a slightly different artificial neuron called a threshold logic unit (TLU), or sometimes a linear threshold unit (LTU). The most common step function used in Perceptrons is the [Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function), sometimes the [sign function](https://en.wikipedia.org/wiki/Sign_function) is used instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kAtJIPTSqQC"
      },
      "source": [
        "We can use the [Perceptron method](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) from sklearn to implement a percentron for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6MlfNSJUgwI"
      },
      "source": [
        "#### **Exercise**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTEGJzYWUmfl"
      },
      "source": [
        "Read the documentation of the [Perceptron method](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) from sklearn and train a clasification model (fit the parameter to max X to y).\n",
        "\n",
        "**Hint**: Look at the \"fit\" method and pay attention to the dimensions of the arrays/inputs!!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV2Q-KPjO4Tw"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
        "\n",
        "#Lets transform the data to a bynary clasification problem (Iris-Setosa or not)\n",
        "y = (iris.target == 0).astype(int).reshape(-1,1)  \n",
        "\n",
        "per_clf = Perceptron(tol=1e-3, random_state=42)\n",
        "\n",
        "###==== START CODE HERE ====### (≈ 1 lines of code)\n",
        "\n",
        "\n",
        "###==== END CODE HERE ====###\n",
        "\n",
        "\n",
        "y_pred = per_clf.predict([X[0,:]])\n",
        "print(\"The predicted outcome is: \",y_pred[0], \"and the ground truth is:\", y[0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76iLgMgAWIxP"
      },
      "source": [
        "**Expected Output**:\n",
        "       \n",
        "```\n",
        "The predicted outcome is:  1 and the ground truth is: 1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7reIsWJ2O4UH"
      },
      "source": [
        "a = -per_clf.coef_[0][0] / per_clf.coef_[0][1]\n",
        "b = -per_clf.intercept_ / per_clf.coef_[0][1]\n",
        "y=y.flatten()\n",
        "\n",
        "axes = [0, 5, 0, 2]\n",
        "\n",
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n",
        "        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "y_predict = per_clf.predict(X_new)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
        "\n",
        "plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\", linewidth=3)\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"lower right\", fontsize=14)\n",
        "plt.axis(axes)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAGkEESuO4UR"
      },
      "source": [
        "# 2- Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mc6gEBCXRMi"
      },
      "source": [
        "We have learned multiple activation functions, like:\n",
        "\n",
        "$$sigmoid(z) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "\n",
        "$$tanh(z) = \\frac{e^{z}- e^{-z}}{e^{z}+ e^{-z}}$$\n",
        "\n",
        "\n",
        "$$ReLu(z) = max{(0,z)}$$\n",
        "\n",
        "\n",
        "$$LeakyReLu(a,z) = max{(az,z)}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QTYsclgqPLz"
      },
      "source": [
        "While we can mathematically get the derivatives of these activation functions, we can also algorithmically calculate the slope of g(z) at any point by taking a very small step to the right and left of z and calculating the slope of the function, like:\n",
        "\n",
        "$$g'(z)=\\frac{g(z+ \\epsilon)-g(z- \\epsilon)}{2 \\times \\epsilon}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9X5L06Z2sTk"
      },
      "source": [
        "#### **Exercise**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXhaJCTC2v0y"
      },
      "source": [
        "Complete the following function declarations: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBDqfCZpO4UU"
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "def tanh(z):\n",
        "###==== START CODE HERE ====### (≈ 1 lines of code)\n",
        "\n",
        "\n",
        "###==== END CODE HERE ====###\n",
        "\n",
        "def ReLu(z):\n",
        "###==== START CODE HERE ====### (≈ 1 lines of code)\n",
        "\n",
        "\n",
        "###==== END CODE HERE ====###\n",
        "\n",
        "def LeakyRelu(z,a=0.1):   #a=0.1 to make it easly to vizualize the differntece, a better values would be 0.001\n",
        "###==== START CODE HERE ====### (≈ 1 lines of code)\n",
        "\n",
        "\n",
        "###==== END CODE HERE ====###\n",
        "\n",
        "def derivative(f, z, eps=0.000001):\n",
        "###==== START CODE HERE ====### (≈ 1 lines of code)\n",
        "\n",
        "\n",
        "###==== END CODE HERE ====###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk8JRI80O4Ud"
      },
      "source": [
        "z = np.linspace(-5, 5, 200)\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(z, np.sign(z), \"r-\", linewidth=1, label=\"Step\")\n",
        "plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
        "plt.plot(z, tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
        "plt.plot(z, ReLu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
        "plt.plot(z, LeakyRelu(z), \"m--\", linewidth=2, label=\"LeakyRelu\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc=\"center right\", fontsize=14)\n",
        "plt.title(\"Activation functions\", fontsize=14)\n",
        "plt.axis([-5, 5, -1.2, 1.2])\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=1, label=\"Step\")\n",
        "plt.plot(0, 0, \"ro\", markersize=5)\n",
        "plt.plot(0, 0, \"rx\", markersize=10)\n",
        "plt.plot(z, derivative(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
        "plt.plot(z, derivative(tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
        "plt.plot(z, derivative(ReLu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
        "plt.plot(z, derivative(LeakyRelu, z), \"m--\", linewidth=2, label=\"LeakyRelu\")\n",
        "plt.grid(True)\n",
        "#plt.legend(loc=\"center right\", fontsize=14)\n",
        "plt.title(\"Derivatives\", fontsize=14)\n",
        "plt.axis([-5, 5, -0.2, 1.2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Aw6A4SwO4Ux"
      },
      "source": [
        "# 3- Building an NN with Keras for Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaVcEOUVYVDL"
      },
      "source": [
        "### 3.1- Importing TensorFlow and Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut3MQvktO4U2"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roQpC8U6O4U7"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81DSzge9O4VB"
      },
      "source": [
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00I89R00YhSq"
      },
      "source": [
        "### 3.2- The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sgaflt2O4VH"
      },
      "source": [
        "Let's start by loading the fashion MNIST dataset. Keras has a number of functions to load popular datasets in `keras.datasets`. The dataset is already split for you between a training set and a test set, but it can be useful to split the training set further to have a validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqAUiXDqO4VJ"
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4t7yVEyO4VO"
      },
      "source": [
        "The training set contains 60,000 grayscale images, each 28x28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcpgVZ0ZO4VP"
      },
      "source": [
        "X_train_full.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGDwy0aO4VU"
      },
      "source": [
        "Each pixel intensity is represented as a byte (0 to 255) since in a 8 bits we can get [total number of 256 possible combinations](https://user.eng.umd.edu/~nsw/chbe250/number.htm):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d0Uzm5fO4VV"
      },
      "source": [
        "X_train_full.dtype   #8-bit unsigned integer =uint8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb9MXu8fYmvC"
      },
      "source": [
        "#### 3.2.1- Splitting the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yom1ZpOkO4VZ"
      },
      "source": [
        "Let's split the full training set into a validation set and a (smaller) training set. We also scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDJQ143LO4Va"
      },
      "source": [
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phsRVGzmO4Vg"
      },
      "source": [
        "You can plot an image using Matplotlib's `imshow()` function, with a `'binary'`\n",
        " color map:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu298vsiO4Vh"
      },
      "source": [
        "plt.imshow(X_train[3], cmap=\"binary\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKfC6XGTO4Vl"
      },
      "source": [
        "The labels are the class IDs (represented as uint8), from 0 to 9:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3mMa9lWO4Vm"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKlvvVvjO4Vr"
      },
      "source": [
        "Here are the corresponding class names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh_a_S08O4Vt"
      },
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uErWHvrJO4Vz"
      },
      "source": [
        "So the 4th image in the training set is a 'Ankle boot':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uPPdDwtO4V1"
      },
      "source": [
        "class_names[y_train[4]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zzjNgvI8LdW"
      },
      "source": [
        "print( \"The training set shape is: \" , X_train.shape)\n",
        "print( \"The validation set shape is: \", X_valid.shape)\n",
        "print( \"The testing set shape is: \", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OORMf7vnYwCg"
      },
      "source": [
        "#### 3.2.2.- Visualizing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3IcA65gO4WC"
      },
      "source": [
        "Let's take a look at a sample of the images in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn3ugDIcO4WD"
      },
      "source": [
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyr7XsKe-SJO"
      },
      "source": [
        "## 3.3- Setting up the NN model with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8FRTvpHZK4h"
      },
      "source": [
        "To ensure reproducibility we need to set the random seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdWdHNRqO4WO"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBI0RNHnZR_J"
      },
      "source": [
        "Now we need to specify our Neural Network architecture. We need to specify:\n",
        "* The number of layers in our network\n",
        "* The number of neurons per layer\n",
        "* The activation function of our layers\n",
        "\n",
        "We can achieve all this by using `keras.layers`. There are multiple types of layer we can use (see more [here](https://keras.io/layers/core/)). For this architecture we will have:\n",
        "\n",
        "**1)** The input layer will be a keras.layers.Flatten in order to \"flatten” a image from a matrix to a vector.\n",
        "\n",
        "**2)** The 1st hidden layer will be a fully/densely connected layer with 300 neurons and ReLu activation function using `keras.layers.Dense(300, activation=\"relu\")`\n",
        "\n",
        "**3)** The 2nd hidden layer will be another fully/densely connected layer with 100 neuron and ReLu activation function using `keras.layers.Dense(100, activation=\"relu\")`\n",
        "\n",
        "**4)** The last layer, our output layer, will be another fully/densely connected layer but with just 10 layers since we have 10 classes. Also, since we have a multiclass classification problem, we will use a softmax activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fwvWjrRO4WR"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE2pVtKKO4WU"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhvVDzDuI5sO"
      },
      "source": [
        "We can get a summary of our NN architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "776rIVEKO4WY"
      },
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1xEdSxOJX_q"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taqv_kZVJhUD"
      },
      "source": [
        "This summary show that:\n",
        "\n",
        "* $n^{[0]}$= 784  (inputs)\n",
        "* $n^{[1]}$= 300\n",
        "* $n^{[2]}$= 100\n",
        "* $n^{[3]}$= 10 (outputs)\n",
        "\n",
        "Hence, we have a total of 266,610 model parameters that need to be trained. Thankfully, Keras already randomly initialize the weights and initialized to zero our bias terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC1zBh7QO4Wm"
      },
      "source": [
        "hidden1 = model.layers[1]\n",
        "hidden1.name\n",
        "weights, biases = hidden1.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ay39MygO4Wr"
      },
      "source": [
        "weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P24MBdG8LWJx"
      },
      "source": [
        "biases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ7qkZNwLazk"
      },
      "source": [
        "Just in case you want to look at the model's parameters, you need to be aware that in Keras the shape/dimension of $W^{[l]}$ is actually $(n^{[l-1]}, n^{[l]})$ and $b^{[l]}$ is  $(n^{[l]},)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn6EsAy4O4Wu"
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7h6Ih3SO4W5"
      },
      "source": [
        "biases.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f74-LYKmNPOz"
      },
      "source": [
        "## 3.4- Training our NN model with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTQw6ntGNXrZ"
      },
      "source": [
        "Since this is a multiclass classification problem we would use the Cross Entropy loss function (see Eq. 4-22) ; however, for our evaluation metric we would like to use [accuracy]( https://developers.google.com/machine-learning/crash-course/classification/accuracy) since we would like to know the percentage of correctly classified instances. Lastly, since we have so many parameters, lets use  Stochastic gradient descent (sgd) optimizer to get to a “good” solution fast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id2g0ssjO4W-"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "#Lets save our initial model weights so we can \"reset\" the traning latter\n",
        "Wsave = model.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYKTL9qgO4XA"
      },
      "source": [
        "This is equivalent to:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCEOej1dO4XB"
      },
      "source": [
        "```python\n",
        "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.SGD(),\n",
        "              metrics=[keras.metrics.sparse_categorical_accuracy])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L79kTIGkL5Nf"
      },
      "source": [
        "Now we can train our model, and fit our parameters to the training set. We also pass the validation set just for evaluation purposes (no training is done with the validation set).\n",
        "\n",
        "For now, lets do 1 iteration (`epochs=1`) of my SGD going over the whole dataset (`batch_size=1`).\n",
        "\n",
        "This will take a while to run..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwJ5DVKZW6WR"
      },
      "source": [
        "tic = time.process_time()\n",
        "model.set_weights(Wsave)\n",
        "history = model.fit(X_train, y_train, epochs=1, batch_size=1, validation_data=(X_valid, y_valid), verbose=1)\n",
        "toc = time.process_time()\n",
        "print(\"Time to train the model:\"+str((toc - tic)) + \"s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTXd7WDhXaBn"
      },
      "source": [
        "If you did not commented out the `model.set_weights(Wsave)` and is the 1st time you run this code cell, from the printout you will see that the model gets a training accuracy of ~81% and validation accuracy of ~80%. Every time me run the `.fit` method of the model, we continue to train the model \n",
        "\n",
        "Let’s see if we can get a better performance by training over more iterations, but in batches now (like BGD), so let’s \"reset\" our model.\n",
        "\n",
        "This will take even longer to run.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QTmgSR5bhFg"
      },
      "source": [
        "tic = time.process_time()\n",
        "model.set_weights(Wsave)\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=50, validation_data=(X_valid, y_valid), verbose=1)\n",
        "toc = time.process_time()\n",
        "print(\"Time to train the model:\"+str((toc - tic)) + \"s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iud8cLaqVZyi"
      },
      "source": [
        "We got a better accuracy now!! but it took so long, in the next notebook we will learn how to train out model using the free GPU of Google Colab.\n",
        "\n",
        "It is important to understand the difference between epochs and how `barch_size` related to the step of our optimizer. As you can see bellow, in our last training, we had `epochs=30` and `steps=1100`. \n",
        "\n",
        "An Epoch is an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. Hence, setting up the number of epochs is like setting up the number of times we want Keras to save the loss and performance metric of my training and validation set. While in each epoch, we go over several steps updating our parameters (as you can see from the update bar), but only calculate loss and performance of validation set at the end of each epoch.\n",
        "\n",
        "The  number of steps are given by the `m/batch size`. We could also use the `steps_per_epoch` to directly set this value.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYqTDRmEO4XI"
      },
      "source": [
        "history.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ9ARdV0bUN1"
      },
      "source": [
        "The `model.fit` method returns a *history* object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sqX6O8YO4XP"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRVslLkobv1H"
      },
      "source": [
        "We can use these values to plot the convergence of our model. Be advise that the x axis is number of epochs, not size of training set as in our Learning Curves plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHBFsLE5O4XT"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "205ixaiJb8I4"
      },
      "source": [
        "We can clearly see that our model is converging just fine. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkZgpMF-cGdO"
      },
      "source": [
        "## 3.5- Using our NN for prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsF2l94idIHJ"
      },
      "source": [
        "Now we can use our trained model and testing set to get an estimate of how our model would perform if deployed. \n",
        "\n",
        "Remember, that you should never look at the testing set performance and re-training your model, since you will be **OVERFITTING TO THE TEST SET!!!**... hence you would not have any data to reliably estimate of how our model would perform if deployed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg_JOnG-O4XZ"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byj3_wHIdszK"
      },
      "source": [
        "We can see that even to get a prediction out of our model, it takes some time. This is a metric that in some cases might be as valuable as accuracy or performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP93ZyK8O4Xb"
      },
      "source": [
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYhAvB0recN4"
      },
      "source": [
        "Since the output later of this NN model has 10 artificial neuron, one for each of the class in my dataset, as output I will get a vector of probabilities. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXz597mXO4Xe"
      },
      "source": [
        "y_prob = model.predict(X_new) \n",
        "y_classes = y_prob.argmax(axis=-1)\n",
        "y_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3WjU_tkO4Xi"
      },
      "source": [
        "np.array(class_names)[y_classes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UaFBdhiO4Xl"
      },
      "source": [
        "y_new = y_test[:3]\n",
        "y_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br1cyu5YO4Xn"
      },
      "source": [
        "plt.figure(figsize=(7.2, 2.4))\n",
        "for index, image in enumerate(X_new):\n",
        "    plt.subplot(1, 3, index + 1)\n",
        "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
        "    plt.axis('off')\n",
        "    plt.title(class_names[y_test[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06B8P6jbg0a8"
      },
      "source": [
        "\n",
        "# 4- Building your own NN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7wHx6hIAWvo"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "fashion_mnist = keras.datasets.mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "class_names = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReFG7NNLnxRO"
      },
      "source": [
        "X_train[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpX67MjVBX6Z"
      },
      "source": [
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWaFPVSAA05w"
      },
      "source": [
        "#### **Exercise**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7-wh76RD4pj"
      },
      "source": [
        "Now use the [MNIST digits]( http://yann.lecun.com/exdb/mnist/) dataset to train a NN model.  Make sure to:\n",
        "-\tPerform any data preprocessing if necessary\n",
        "-\tMake sure your model does not take more than 5mins to train (using CPU). See the training time at the end.\n",
        "-\tMake sure your results are reproducible \n",
        "-\tMake sure to plot the Loss and performance metric of your model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKC5NtcOCs4n"
      },
      "source": [
        "tic = time.process_time()\n",
        "###===================================== START CODE HERE ========================### (≈ 35ish lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###==== END CODE HERE ====###\n",
        "\n",
        "toc = time.process_time()\n",
        "total_time=toc-tic\n",
        "print(\"Time to train the model:\"+str((total_time)) + \"s\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQOiBJe7CtXe"
      },
      "source": [
        "#### 4.1- Model’s Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwJblbg4Cn1y"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8U3DpZErAxU"
      },
      "source": [
        "y_proba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKN0OrTyC02I"
      },
      "source": [
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "\n",
        "\n",
        "\n",
        "y_prob = model.predict(X_new) \n",
        "y_pred = y_prob.argmax(axis=-1)\n",
        "plt.figure(figsize=(7.2, 2.4))\n",
        "\n",
        "y_new = y_test[:3 ]\n",
        "\n",
        "for index, image in enumerate(X_new):\n",
        "    plt.subplot(1, 3, index + 1)\n",
        "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
        "    plt.axis('off')\n",
        "    plt.title(class_names[y_pred[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy5f7p4SGizB"
      },
      "source": [
        "###### **DO NOT DELETE NOR MODIFY THESE CODE CELLS**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5_oXq-aGiij"
      },
      "source": [
        "\n",
        "# # ###DO NOT DELETE NOR MODIFY THIS CODE CELL####\n",
        "!git clone https://github.com/lopezbec/Intro_toNN_Keras\n",
        "%cd Intro_toNN_Keras\n",
        "from GRADING import GRADING\n",
        "\n",
        "try:\n",
        "    per_clf\n",
        "except:\n",
        "    per_clf=None\n",
        "try:\n",
        "    tanh\n",
        "except:\n",
        "    tanh=None\n",
        "try:\n",
        "    ReLu\n",
        "except:\n",
        "    ReLu=None\n",
        "try:\n",
        "    LeakyRelu\n",
        "except:\n",
        "    LeakyRelu=None\n",
        "try:\n",
        "    derivative\n",
        "except:\n",
        "    derivative=None\n",
        "try:\n",
        "    X_train\n",
        "except:\n",
        "    X_train=None\n",
        "try:\n",
        "    total_time\n",
        "except:\n",
        "    total_time=None\n",
        "\n",
        "GRADING(per_clf,tanh,ReLu,LeakyRelu,derivative,X_train)\n",
        "print(\"TIME <5?=\",total_time<5*60)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}