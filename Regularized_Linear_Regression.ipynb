{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvCaYYygG0ak"
      },
      "source": [
        "<center><b>© Content is made available under the CC-BY-NC-ND 4.0 license. Christian Lopez, lopezbec@lafayette.edu<center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWckiY_RphgE"
      },
      "source": [
        "#Introduction to Regularized Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI-kbu-zojyb"
      },
      "source": [
        " This notebook will give you a brief introduction to the concept of Regularization, with a focus on Regularized Linear Regressio.\n",
        " \n",
        "Most of the notebooks we are going to be using are inspired from existing notebooks that available online and are made free for educational purposes. Nonetheless, this notebook should not be share without prior permission of the instructor. When working in an assignment always remember the Student Code of Conduct. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAcX_pPPg2yw"
      },
      "source": [
        "*The following sections were inspired and uses code and text from the notebook:\n",
        "\n",
        "Géron, A. (2019)   2nd Ed. Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. O'Reilly Media, Inc.( ISBN-10: 1491962291) [Chapter 4](https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb)\n",
        "\n",
        "\n",
        "VanderPlas, J. (2016). Python data science handbook: Essential tools for working with data.  O'Reilly Media, Inc. (ISBN-10 1491912057) [Chapter 5](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.06-Linear-Regression.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QwasjN6GMgL"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFsdePPeGMgk"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "#Scikit-learn for implemeting LinearRegression from a existing algorithm.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "def computeCost(X, y, theta):\n",
        "    return 1/(2*y.size)*np.sum(np.square(X.dot(theta)-y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuhNLwSGlu8n"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "El3AT0Ma1BQw"
      },
      "source": [
        "np.random.seed(42)\n",
        "m = 20\n",
        "X = 3 * np.random.rand(m, 1)\n",
        "y = 1 + 0.5 * X + X**2 + np.random.randn(m, 1) / 1.5\n",
        "X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
        "\n",
        "\n",
        "Degree_of_the_Polynomial_Model=13 #@param {type:\"integer\", min:1, max:14, step:1}\n",
        "poly_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "#Fit model\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "style=\"g-\" \n",
        "width=1\n",
        "polybig_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\n",
        "std_scaler = StandardScaler()\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "\n",
        "polynomial_regression = Pipeline([\n",
        "        (\"poly_features\", polybig_features),\n",
        "        (\"std_scaler\", std_scaler),\n",
        "        (\"lin_reg\", lin_reg),\n",
        "    ])\n",
        "polynomial_regression.fit(X, y)\n",
        "y_newbig = polynomial_regression.predict(X_new)\n",
        "\n",
        "plt.plot(X_new, y_newbig, \"r\", linewidth=width)\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy6dB3H_11aO"
      },
      "source": [
        "Adding more term to our model improves the MSE, but it could impact generalizability. A way to mitigate this issue is using Regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44dtjHga26O4"
      },
      "source": [
        "## 1- Ridge Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NnBRtgs3Bu8"
      },
      "source": [
        "\n",
        "Perhaps the most common form of regularization is known as *ridge regression* or $L_2$ *regularization*, sometimes also called [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization).\n",
        "This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be \n",
        "$$\n",
        "P = \\lambda\\sum_{j=1}^N \\theta_j^2\n",
        "$$\n",
        "Where $\\lambda$ is a free parameter that controls the strength of the penalty. \n",
        "\n",
        "The $\\lambda$ parameter is essentially a knob controlling the complexity of the resulting model.\n",
        "\n",
        "In the limit $\\lambda \\to 0$, we recover the standard linear regression result; in the limit $\\lambda \\to \\infty$, all model responses will be suppressed.\n",
        "\n",
        "One advantage of ridge regression in particular is that it can be computed very efficiently—at hardly more computational cost than the original linear regression model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX3TFLhTHsxZ"
      },
      "source": [
        "\n",
        "### 1.2 - Ridge Regression Implementation \n",
        "\n",
        "The objective of Ridge Linear Regression is to minimize the cost function as shown below, which is similar to the cost function for the non-regularized linear regression, but with the additional regularization term!.\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2 + \\lambda\\sum_{j=1}^N \\theta_j^2 \\right]$$\n",
        "\n",
        "\n",
        "\n",
        "Sice we don't want to regulazied our $\\theta_0$, just our $\\theta_j$ for $j=1,2,3,...,n$, the update rules becomes:\n",
        "\n",
        "$$ \\theta_0 = \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)$$\n",
        "\n",
        "$$ \\theta_j = \\theta_j * (1-\\alpha \\frac{\\lambda}{m} ) - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)}\\quad     \\forall\\quad   j=\\{1,2,...,N\\} $$  \n",
        "\n",
        "$$ \\qquad \\text{simultaneously update }$$\n",
        "\n",
        "\n",
        "The vectorized implementations is:\n",
        "\n",
        "$$ \\theta = \\theta  (1- \\alpha\\frac{ λ }{m})-(\\frac{\\alpha }{m})\\ X^{T}(X \\theta -Y)\\$$\n",
        "\n",
        "From the above implementation you will update $\\theta_0$ as:\n",
        "\n",
        "$$ \\theta_0 = \\theta_0 - (\\frac{\\alpha }{m}) \\ X^{T}(X \\theta -Y)\\$$\n",
        "\n",
        "\n",
        "Another way of doing all this in a single step  is:\n",
        "$$ \\theta = \\theta  (1- (\\alpha\\frac{ λ }{m}) A)-(\\frac{\\alpha }{m})\\ X^{T}(X \\theta -Y)\\$$\n",
        "\n",
        "\n",
        "Where $A$ is the $(n \\times 1)$ matrix full of ones, with a single 0 in its first entry\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J95hAVoRlv9"
      },
      "source": [
        "def RidgeRegression_gradientDescent(X, y, theta, alpha, lambda_term, num_iters):\n",
        "    \"\"\"\n",
        "    Performs Ridge Linear Regression with a vectorized gradient descent to learn `thetas`. Updates theta by taking `num_iters`\n",
        "    gradient steps with learning rate `alpha`.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The input dataset of shape (m x n+1).\n",
        "    \n",
        "    y : arra_like\n",
        "        Value at given features. A vector of shape (m x 1 ).\n",
        "    \n",
        "    theta : array_like\n",
        "        Initial values for the linear regression parameters. \n",
        "        A vector of shape (n+1 x 1).\n",
        "    \n",
        "    alpha : float\n",
        "        The learning rate.\n",
        "\n",
        "    lambda_term : float\n",
        "        The regularization term.\n",
        "    \n",
        "    num_iters : int\n",
        "        The number of iterations for gradient descent. \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    theta : array_like\n",
        "        The learned linear regression parameters. A vector of shape (n+1 x 1 ).\n",
        "    \n",
        "    J_history : list\n",
        "        A python list for the values of the cost function after each iteration.\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialize some useful values\n",
        "    m = y.shape[0]  # number of training examples\n",
        "    \n",
        "    # make a copy of theta, to avoid changing the original array, since numpy arrays\n",
        "    # are passed by reference to functions!!!\n",
        "    theta = theta.copy()\n",
        "    \n",
        "    J_history = [] # Use a python list to save cost in every iteration\n",
        "    ### START CODE HERE ### (≈ 5 lines of code)\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "        J_history.append(computeCost(X, y, theta))\n",
        "    return theta, J_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBpJGDLmC8mV"
      },
      "source": [
        "Lets test your code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC4s2LVKTTMn",
        "cellView": "both"
      },
      "source": [
        "\n",
        "#Create polynomial features\n",
        "Degree_of_the_Polynomial_Model=5\n",
        "#\"include_bias=False\" since we dont want to normalized the intercep/bias term\n",
        "poly_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "#Get the Mean and SD used to normalize the data, so we can apply it to our testing\n",
        "X_scale=std_scaler.fit(X_poly)\n",
        "\n",
        "#Scaling our data is key for Regularized LR to work\n",
        "X_poly=std_scaler.fit_transform(X_poly)\n",
        "#If you comment the line above you will see gradient descent will not work well (or at all)\n",
        "X_poly = np.hstack([np.ones(shape=(y.size,1)), X_poly])\n",
        "\n",
        "# initialize fitting parameters (n+1)\n",
        "theta= np.zeros(Degree_of_the_Polynomial_Model+1).reshape(Degree_of_the_Polynomial_Model+1,1)\n",
        "\n",
        "# some gradient descent settings\n",
        "num_iters = 1000  #8000\n",
        "alpha = 0.1      #0.3\n",
        "lambda_term=0.02\n",
        "\n",
        "theta, J_history = RidgeRegression_gradientDescent(X_poly ,y, theta, alpha, lambda_term,num_iters)\n",
        "\n",
        "print(theta)\n",
        "theta_cost=computeCost(X_poly, y, theta)\n",
        "print(\"Cost value= {}\".format(round(theta_cost,4)))\n",
        "\n",
        "#You can plot your  cost over the gradient descent iterations\n",
        "# plt.plot(J_history)\n",
        "# plt.xlim(xmin=0.0)\n",
        "# plt.ylabel('Cost J')\n",
        "# plt.xlabel('Iterations');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE9xisadxvJ8"
      },
      "source": [
        "**Expected output (approx look at cost value):**\n",
        "\n",
        "For Degrees=5, alpha=0.1, num_iters=1000, lambda_term=0.02\n",
        "```\n",
        "[[4.19953864]\n",
        " [1.16239954]\n",
        " [0.61418251]\n",
        " [0.55422587]\n",
        " [0.47859823]\n",
        " [0.41687117]]\n",
        "Cost value= 0.1423\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XrkRzdyDu38"
      },
      "source": [
        "## 1.3 - Ridge Regression with scikit-learn "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT9F8M2mL_Cg"
      },
      "source": [
        "Here is the code on how to run the same Ridge Linear Regression using scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B1X-zAQrY1q"
      },
      "source": [
        "#Import Ridge Linear Regression\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "#Set the hyperparameter\n",
        "ridge_reg = Ridge(alpha=lambda_term)\n",
        "\n",
        "#Lets pass out X matrix with the polynomial terms\n",
        "ridge_reg.fit(X_poly, y)\n",
        "\n",
        "#Lets get the thetas\n",
        "coe=ridge_reg.coef_\n",
        "interc=ridge_reg.intercept_\n",
        "coe=coe.reshape(coe.size,1)\n",
        "interc=interc.reshape(interc.size,1)\n",
        "theta_sklearn=np.vstack([interc,coe[1:, :]])\n",
        "print(theta_sklearn)\n",
        "theta_sklearn_cost=computeCost(X_poly, y, theta_sklearn)\n",
        "print(\"Cost value= {}\".format(round(theta_sklearn_cost,4)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKRk_xpXwSrF"
      },
      "source": [
        "Lets plot our  models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBGOdlObR0Z2"
      },
      "source": [
        "#Lets create some new data to create a line\n",
        "X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
        "#Add some polynomial term to this data\n",
        "poly_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\n",
        "X_new_poly = poly_features.fit_transform(X_new)\n",
        "\n",
        "#WE NEED TO SCALE OUR DATA BASED ON THE SCALE WE USE FOR TRAINING!!!!\n",
        "X_new_poly_sclae=X_scale.transform(X_new_poly)\n",
        "\n",
        "#Add Theta_0\n",
        "X_new_poly_sclae_0 = np.hstack([np.ones(shape=(X_new.size,1)), X_new_poly_sclae])\n",
        "\n",
        "#Lets cacuate our model (h, y_hat)\n",
        "y_newbig_theta= X_new_poly_sclae_0.dot(theta)\n",
        "y_newbig_theta_sklearn= X_new_poly_sclae_0.dot(theta_sklearn)\n",
        "\n",
        "\n",
        "#Plot models\n",
        "plt.plot(X_new, y_newbig_theta, \"r\", linewidth=width, label='Our Implementation')\n",
        "plt.plot(X_new, y_newbig_theta_sklearn, \"g\", linewidth=width,  label='Sklearn Implementation')\n",
        "\n",
        "\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.xlabel(\"$x$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=4);\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtNRKfZyPv66"
      },
      "source": [
        "#### Ridge Regression hyperparameters: degree of polynomial and $\\lambda$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCb5S_Y4Ecav",
        "cellView": "form"
      },
      "source": [
        "X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
        "style=\"g-\" \n",
        "width=1\n",
        "Degree_of_the_Polynomial_Model=50 #@param {type:\"integer\", min:1, max:14, step:1}\n",
        "lambda_term=0.02  #@param \n",
        "polybig_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\n",
        "std_scaler = StandardScaler()\n",
        "Ridge_lin_reg = Ridge(alpha=lambda_term)\n",
        "\n",
        "\n",
        "polynomial_ridge_regression = Pipeline([\n",
        "        (\"poly_features\", polybig_features),\n",
        "        (\"std_scaler\", std_scaler),\n",
        "        (\"Ridge_lin_reg\", Ridge_lin_reg),\n",
        "    ])\n",
        "polynomial_ridge_regression.fit(X, y)\n",
        "y_newbig = polynomial_ridge_regression.predict(X_new)\n",
        "\n",
        "plt.plot(X_new, y_newbig, \"r\", linewidth=width)\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE60Hf2ryvs9"
      },
      "source": [
        "## 2- Lasso Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IofK68-ryWuy"
      },
      "source": [
        "\n",
        "Least Absolute Shrinkage and Selection Operator Regression (usually simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm of the weight vector instead of half the square of the ℓ2 norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGt9OfK5JhYZ"
      },
      "source": [
        "The objective of Lasso Linear Regression is to minimize the cost function as shown below, which is similar to the cost function for the non-regularized linear regression, but with the additional regularization term (ℓ1 norm).\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2 + \\lambda\\sum_{j=1}^N |\\theta_j|\\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vutsr8BK8Ws"
      },
      "source": [
        "The Lasso cost function is not differentiable at $\\theta_i$ = 0 (for i = 1, 2, ⋯, n), but Gradient Descent still works fine if you use a subgradient vector instead (see Hands-on Machine Learning book  2nd ed, pag 140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9htxANLsM33-"
      },
      "source": [
        "## 2.1 - Lasso Regression with scikit-learn "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j4xXxJ9K8LE"
      },
      "source": [
        "#Import Lasso  Linear Regression\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "#Set the hyperparameter\n",
        "lasso_reg = Lasso(alpha=lambda_term)\n",
        "\n",
        "#Lets pass out X matrix with the polynomial terms\n",
        "lasso_reg.fit(X_poly, y)\n",
        "\n",
        "#Lets get the thetas\n",
        "coe=lasso_reg.coef_\n",
        "interc=lasso_reg.intercept_\n",
        "coe=coe.reshape(coe.size,1)\n",
        "interc=interc.reshape(interc.size,1)\n",
        "theta_sklearn=np.vstack([interc,coe[1:, :]])\n",
        "print(theta_sklearn)\n",
        "theta_sklearn_cost=computeCost(X_poly, y, theta_sklearn)\n",
        "print(\"Cost value= {}\".format(round(theta_sklearn_cost,4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JybB9u_Nh3v"
      },
      "source": [
        "#### Lasso Regression hyperparameters: degree of polynomial and $\\lambda$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u07jsH0NmvV",
        "cellView": "form"
      },
      "source": [
        "X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
        "\n",
        "style=\"g-\" \n",
        "width=1\n",
        "\n",
        "Degree_of_the_Polynomial_Model=10 #@param {type:\"integer\", min:1, max:14, step:1}\n",
        "lambda_term=0.02  #@param \n",
        "\n",
        "polybig_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\n",
        "std_scaler = StandardScaler()\n",
        "lasso_reg = Lasso(alpha=lambda_term)\n",
        "\n",
        "polynomial_lasso_regression = Pipeline([\n",
        "        (\"poly_features\", polybig_features),\n",
        "        (\"std_scaler\", std_scaler),\n",
        "        (\"lasso_reg\", lasso_reg),\n",
        "    ])\n",
        "polynomial_lasso_regression.fit(X, y)\n",
        "y_newbig = polynomial_lasso_regression.predict(X_new)\n",
        "\n",
        "plt.plot(X_new, y_newbig, \"r\", linewidth=width)\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "172W_A8lPTk1"
      },
      "source": [
        "### So when should you use plain Linear Regression (i.e., without any regularization),Ridge, Lasso, or Elastic Net? \n",
        "\n",
        "It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are useful, you should prefer Lasso or Elastic Net because they tend to reduce the useless features’ weights down to zero, as we have discussed."
      ]
    }
  ]
}