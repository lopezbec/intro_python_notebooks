{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pySxNIJ8ZhE"
      },
      "source": [
        "<center><b>© Content is made available under the CC-BY-NC-ND 4.0 license. Christian Lopez, lopezbec@lafayette.edu<center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/lopezbec/intro_python_notebooks/blob/master/Softmax_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td>\n",
        "         </table>\n",
        "          <br><br></br> "
      ],
      "metadata": {
        "id": "FZu8dq7f85-8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cekq7VLGUAui"
      },
      "source": [
        "# Softmax and Early Stopping Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfz58eKB8t6f"
      },
      "source": [
        "In this assigment you will implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4HIDlsrUAu8"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muQrurkWUAvL"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import time \n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFKH-4PiUA27"
      },
      "source": [
        "## Batch Gradient Descent with early stopping for Softmax Regression\n",
        "(without using Scikit-Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qXrfInuUA28"
      },
      "source": [
        "Let's start by loading the data. We will use the Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmF7RmXX_EzE"
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfVwmBUz_Mlw"
      },
      "source": [
        "print(iris.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGDdfAhqUA29"
      },
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49SakiYnUA2_"
      },
      "source": [
        "We need to add the bias term for every instance ($x_0 = 1$):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCpxk8_QUA2_"
      },
      "source": [
        "X_with_bias = np.c_[np.ones([len(X), 1]), X]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH-soBuIUA3B"
      },
      "source": [
        "And let's set the random seed so the output of this exercise solution is reproducible:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9gdtQvrUA3D"
      },
      "source": [
        "np.random.seed(21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcb8mYLjUA3G"
      },
      "source": [
        "The easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn's `train_test_split()` function, but the point of this exercise is to try understand the algorithms by implementing them manually. So here is one possible implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUlPwEuWUA3G"
      },
      "source": [
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "rnd_indices = np.random.permutation(total_size)\n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = y[rnd_indices[-test_size:]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8l7pFtyUA3I"
      },
      "source": [
        "The targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for ay given instance is a one-hot vector). Let's write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnHQj-18UA3J"
      },
      "source": [
        "def to_one_hot(y):\n",
        "    n_classes = y.max() + 1\n",
        "    m = len(y)\n",
        "    Y_one_hot = np.zeros((m, n_classes))\n",
        "    Y_one_hot[np.arange(m), y] = 1\n",
        "    return Y_one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc8IE7KWUA3L"
      },
      "source": [
        "Let's test this function on the first 10 instances:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r5Ap5cNUA3N"
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r97Rukc-UA3O"
      },
      "source": [
        "to_one_hot(y_train[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFch3gRhUA3Q"
      },
      "source": [
        "Looks good, so let's create the target class probabilities matrix for the training set and the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pzb3H9gUA3R"
      },
      "source": [
        "Y_train_one_hot = to_one_hot(y_train)\n",
        "Y_valid_one_hot = to_one_hot(y_valid)\n",
        "Y_test_one_hot = to_one_hot(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BKgD8fiUA3S"
      },
      "source": [
        "Now let's implement the Softmax function. Recall that it is defined by the following equation:\n",
        "\n",
        "$\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}$\n",
        "\n",
        "*Be advise when using `np.sum` which axis you are summing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysxr-KbiUA3S"
      },
      "source": [
        "def softmax(logits):\n",
        "    \"\"\"\n",
        "    Computes the Softmax function. The output is the probability \n",
        "    that the instance belongs to class k. \n",
        "    Parameters\n",
        "    ----------\n",
        "    Logists : array_like\n",
        "        The Softmax score (m x k).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    Softmax probability : array_like\n",
        "    Probability that the instance belongs to class k (m x k).\n",
        "\n",
        "    Instructions\n",
        "    ------------\n",
        "    Remember that you can use np.sum to calculate the \n",
        "    sum by colums or by rows.\n",
        "    \"\"\"\n",
        "\n",
        "    ###========== START CODE HERE=========== ### (≈ 2 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###========== END CODE HERE============= ### \n",
        "\n",
        "    return exps / exp_sums"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shNQMSYVUA3X"
      },
      "source": [
        "We are almost ready to start training. Let's define the number of inputs and outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koflQ-EZUA3Y"
      },
      "source": [
        "n_inputs = X_train.shape[1] # == 3 (2 features plus the bias term)\n",
        "n_outputs = len(np.unique(y_train))   # == 3 (3 iris classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmeavu2PMAgy"
      },
      "source": [
        "Also, lets test your softmax functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm36Sw5pMDdJ"
      },
      "source": [
        "Theta = np.zeros( (n_inputs, n_outputs))\n",
        "Y_proba = softmax(X_train.dot(Theta))\n",
        "print(Y_proba[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFCmVSyQMnh9"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "\n",
        "```\n",
        "[0.33333333 0.33333333 0.33333333]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8xe6GD3UA3a"
      },
      "source": [
        "Now here comes the hardest part: training! Theoretically, it's simple: it's just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it's easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it's working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won't have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what's going on under the hood.\n",
        "\n",
        "So the equations we will need are the cost function:\n",
        "\n",
        "$J(\\mathbf{\\Theta}) =\n",
        "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
        "\n",
        "And the equation for the gradients:\n",
        "\n",
        "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$\n",
        "\n",
        "Note that $\\log\\left(\\hat{p}_k^{(i)}\\right)$ may not be computable if $\\hat{p}_k^{(i)} = 0$. So we will add a tiny value $\\epsilon$ `epsilon`to $\\log\\left(\\hat{p}_k^{(i)}\\right)$ to avoid getting `nan` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_0wFTZcUA3a"
      },
      "source": [
        "alpha = 0.01\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "\n",
        "Theta = np.zeros( (n_inputs, n_outputs))\n",
        "tic = time.process_time()\n",
        "for iteration in range(n_iterations):\n",
        "    ###========== START CODE HERE=========== ### (≈ 4 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###========== END CODE HERE============= ### \n",
        "\n",
        "    if iteration % 1000 == 0:\n",
        "        print(iteration, cost)\n",
        "toc = time.process_time() \n",
        "print(\"Time to run:\"+str(1000*(toc - tic)) + \"ms\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rzJzh7Kjplb"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "\n",
        "```\n",
        "0 1.098611988668155\n",
        "1000 0.6398970617220827\n",
        "2000 0.505762151375006\n",
        "3000 0.43918073481726827\n",
        "4000 0.3974762038411762\n",
        "5000 0.3679158087748432\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yBY3yfJUA3c"
      },
      "source": [
        "And that's it! The Softmax model is trained. Let's look at the model parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8RCT2xklnl_"
      },
      "source": [
        "Theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohFf6bGiUA3i"
      },
      "source": [
        "Let's make predictions for the traning and validation set and check the accuracy score:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Data Accuracy"
      ],
      "metadata": {
        "id": "d-6XFb55V3kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = X_train.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "np.mean(y_predict == y_train)\n"
      ],
      "metadata": {
        "id": "v1hKQRJ-Tpx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Data Accuracy"
      ],
      "metadata": {
        "id": "Cj5NfvVZV-vY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xXHtp0eUA3i"
      },
      "source": [
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score_1 = np.mean(y_predict == y_valid)\n",
        "accuracy_score_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVuyCZ_Sjv04"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "\n",
        "```\n",
        "0.93333\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZXaDQZcUA3k"
      },
      "source": [
        "Well, this model is  doing pretty well on the validation set. However, since the performance on th traning set is larger than the validaiton set it might indicate some potential \"overfitting\". For the sake of the exercise, let's add a bit of $\\ell_2$ regularization. The following training code is similar to the one above, but the loss now has an additional $\\ell_2$ penalty, and the gradients have the proper additional term (note that we don't regularize the first element of `Theta` since this corresponds to the bias term). Also, let's try increasing the learning rate `alpha`.\n",
        "\n",
        "So the equations from above with the regularization term would look like:\n",
        "\n",
        "$J(\\mathbf{\\Theta}) =\n",
        "- \\dfrac{1}{m}[\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}] + \\dfrac{λ}{2m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{(\\theta_j^k)^2}$\n",
        "\n",
        "We add a \"regularization penalty\" that will make the cost goes up as we have larger/more theta. Remember that the '-' sign of the \"categorical cross entropy\"/Softmax cost function, is to make the \"log\" negative (i.e, -log(x) approaches +infinity as x approaches 0)\n",
        "\n",
        "And the equation for the gradients:\n",
        "\n",
        "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m}[ \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}+  λ(\\theta_j^k)}] $\n",
        "\n",
        "Note that we SHOULD NOT regularized  any $\\theta_0$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43_u_RzgUA3k"
      },
      "source": [
        "alpha = 0.01\n",
        "n_iterations = 5001\n",
        "lambda_ = 5 # regularization hyperparameter\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "\n",
        "\n",
        "Theta = np.zeros( (n_inputs, n_outputs))\n",
        "tic = time.process_time() \n",
        "for iteration in range(n_iterations):\n",
        "    ###========== START CODE HERE=========== ### (≈ 7 lines of code)\n",
        "    #Cost\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #Gradient\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###========== END CODE HERE============= ### \n",
        "    if iteration % 1000 == 0:\n",
        "        print(iteration, Cost)\n",
        "\n",
        "toc = time.process_time() \n",
        "print(\"Time to run:\"+str(1000*(toc - tic)) + \"ms\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhwiI7J0j04U"
      },
      "source": [
        "**Expected Output (greater than the non-regularized):**\n",
        "\n",
        "\n",
        "```\n",
        "0 1.098611988668155\n",
        "1000 0.6734854832141064\n",
        "2000 0.5697650172104203\n",
        "3000 0.5250791177625824\n",
        "4000 0.5000702104746237\n",
        "5000 0.483894365030869\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMWrpWSCUA3l"
      },
      "source": [
        "Because of the additional $\\ell_2$ penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let's find out:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Data Accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "wetisWMkWbwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = X_train.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "np.mean(y_predict == y_train)\n"
      ],
      "metadata": {
        "id": "gr6CZunwVHvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Data Accuracy\n"
      ],
      "metadata": {
        "id": "EqX4UVIVYAM-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNV6-XGKUA3m"
      },
      "source": [
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score_l2 = np.mean(y_predict == y_valid)\n",
        "accuracy_score_l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcnzs1zukADM"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "\n",
        "```\n",
        "1.0\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srEkmIluUA3n"
      },
      "source": [
        "Perfect  accuracy! This was just because we got lucky with out Validation Dataset. If you change the randon seed you will see we would get a different value due to the random partition of our dataset). However, if you do so, make sure to change it back to the original random seed value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m2MkbbiUA3o"
      },
      "source": [
        "Now let's add early stopping with $\\ell_2$ regularization. For this, we just need to measure the `cost` on the validation set at every iteration and stop when this error/cost starts growing. This part is already implemented in the code chunk below (see ` if cost < best_cost:...`). Hence, you just need to:\n",
        "\n",
        "\n",
        "1. For each iteration fit the parameters using your training data\n",
        "2. Calculate the cost of that candidate model when evaluated on the validation dataset.\n",
        " \n",
        "  This has to be saved in a variable called `val_cost`. Don’t forget to add the regularization penalty! Since this simple model is doing very well in this \"simple\" dataset, we would need change some of the hyperparameters to see the early stopping working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QPhdZzFUA3o"
      },
      "source": [
        "alpha = 0.1\n",
        "n_iterations = 50001\n",
        "lambda_ = 8 # regularization hyperparameter\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "best_cost = np.infty\n",
        "\n",
        "Theta = np.zeros( (n_inputs, n_outputs))\n",
        "tic = time.process_time() \n",
        "for iteration in range(n_iterations):\n",
        "    ###========== START CODE HERE=========== ### (≈ 12 lines of code)\n",
        "\n",
        "    #SAME AS ABOVE\n",
        "    #Cost\n",
        "\n",
        "\n",
        "    #Gradient\n",
        "\n",
        "\n",
        "\n",
        "    #CODE FOR EARLY STOPPING\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###========== END CODE HERE============= ### \n",
        "\n",
        "    if iteration % 5000 == 0:\n",
        "        print(iteration, val_cost)\n",
        "    if val_cost < best_cost:\n",
        "        best_cost = val_cost\n",
        "    else:\n",
        "        print(iteration - 1, best_cost)\n",
        "        print(iteration, val_cost, \"early stopping!\")\n",
        "        break\n",
        "\n",
        "toc = time.process_time() \n",
        "print(\"Time to run:\"+str(1000*(toc - tic)) + \"ms\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n5h461GkOIX"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "\n",
        "```\n",
        "0 1.0118807259655547\n",
        "5000 0.20967842354493088\n",
        "10000 0.20463280084345567\n",
        "15000 0.20449848791942596\n",
        "20000 0.20449487958054496\n",
        "25000 0.20449478261828707\n",
        "30000 0.2044947800127266\n",
        "35000 0.20449477994271265\n",
        "38004 0.2044947799409999\n",
        "38005 0.20449477994099993 early stopping!\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmRlUWdVUA3r"
      },
      "source": [
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score_estop = np.mean(y_predict == y_valid)\n",
        "accuracy_score_estop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-MRwuAXkQu6"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "\n",
        "```\n",
        "1.0\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXP0rnH6UA31"
      },
      "source": [
        "Still the same accuracy, however, for datasets with more complex \"pattern\" early stopping might help you find the \"best\" model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYUOvqNmUA31"
      },
      "source": [
        "Now let's plot the model's predictions on the whole dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5WPl0fdUA32"
      },
      "source": [
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "X_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new]\n",
        "\n",
        "logits = X_new_with_bias.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "zz1 = Y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1z5bWzYUA33"
      },
      "source": [
        "And now let's measure the final model's accuracy on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgoRJzV1UA34"
      },
      "source": [
        "logits = X_test.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "accuracy_score_final = np.mean(y_predict == y_test)\n",
        "accuracy_score_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlhZbuQuUA36"
      },
      "source": [
        "Our perfect model turns out to have slight imperfections. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISTpQe0xHoaq"
      },
      "source": [
        "###### **DO NOT DELETE NOR MODIFY THESE CODE CELLS**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONpWv6SDHnoR"
      },
      "source": [
        "# # ###DO NOT DELETE NOR MODIFY THIS CODE CELL####\n",
        "!wget https://raw.githubusercontent.com/lopezbec/intro_python_notebooks/main/GRADING_SoftM.py\n",
        "from GRADING_SoftM import GRADING_SoftM\n",
        "\n",
        "\n",
        "try:\n",
        "    accuracy_score_1\n",
        "except:\n",
        "    accuracy_score_1=None\n",
        "try:\n",
        "    accuracy_score_l2\n",
        "except:\n",
        "    accuracy_score_l2=None\n",
        "try:\n",
        "    accuracy_score_estop\n",
        "except:\n",
        "    accuracy_score_estop=None\n",
        "try:\n",
        "    accuracy_score_final\n",
        "except:\n",
        "    accuracy_score_final=None\n",
        "\n",
        "GRADING_SoftM(accuracy_score_1, accuracy_score_l2,accuracy_score_estop,accuracy_score_final)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}